# 爬虫基础

## 爬虫

- 从本质上来说，就是利用程序在网上拿到对我们有价值的数据。

## 传统拿去数据的做法

- 通过浏览器上网，手动下载所需要的数据。其实在这背后，浏览器做了很多我们看不见的工作，而只有了解浏览器的工作原理后，才能真正理解爬虫在帮我们做什么。

## 浏览器的工作原理

- 首先，我们在浏览器输入网址（也可以叫URL）。然后，浏览器向服务器传达了我们想访问某个网页的需求，这个过程就叫做【请求】。
- 紧接着，服务器把你想要的网站数据发送给浏览器，这个过程叫做【响应】。
- 所以浏览器和服务器之间，先请求，后响应，有这么一层关系。
- 当服务器把数据响应给浏览器之后，浏览器并不会直接把数据丢给你。因为这些数据是用计算机的语言写的，浏览器还要把这些数据翻译成你能看得懂的样子，这是浏览器做的另一项工作【解析数据】。
- 紧接着，我们就可以在拿到的数据中，挑选出对我们有用的数据，这是【提取数据】。
- 最后，我们把这些有用的数据保存好，这是【存储数据】。
- 以上，就是浏览器的工作原理，是人、浏览器、服务器三者之间的交流过程。

## 爬虫的工作原理

- 当你决定去某个网页后，首先，爬虫可以模拟浏览器去向服务器发出请求；其次，等服务器响应后，爬虫程序还可以代替浏览器帮我们解析数据；接着，爬虫可以根据我们设定的规则批量提取相关数据，而不需要我们去手动提取；最后，爬虫可以批量地把数据存储到本地。
- 其实，还可以把最开始的【请求——响应】封装为一个步骤——获取数据。由此，我们得出，爬虫的工作分为四步：

|              |                        爬虫的四个步骤                        |
| :----------: | :----------------------------------------------------------: |
| 一. 获取数据 | 爬虫程序会根据我们提供的网址，向服务器发起请求，然后返回数据。 |
| 二. 解析数据 |     爬虫程序会把服务器返回的数据解析成我们能读懂的格式。     |
| 三. 提取数据 |             爬虫程序再从中提取出我们需要的数据。             |
| 四. 存储数据 |  爬虫程序把这些有用的数据保存起来，便于你日后的使用和分析。  |

# 体验爬虫

- 这一部分的任务就是学会爬虫的第0步：获取数据。
- 我们将会利用一个强大的库——`requests`来获取数据。
- `requests`库可以帮我们下载网页源代码、文本、图片，甚至是音频。其实，“下载”本质上是向服务器发送请求并得到响应。

## `requests.get()`

- `requests.get()`的具体用法如下，请仔细阅读注释噢：

```python
import requests
#引入requests库
res = requests.get('URL')
#requests.get是在调用requests库中的get()方法，它向服务器发送了一个请求，括号里的参数是你需要的数据所在的网址，然后服务器对请求作出了响应。
#我们把这个响应返回的结果赋值在变量res上。
```

- 现在，试着用`requests.get()`来下载一个小说——《三国演义》：
- 小说的URL（网址）是：https://localprod.pandateacher.com/python-manuscript/crawler-html/sanguo.md

```python
import requests
res = requests.get("https://localprod.pandateacher.com/python-manuscript/crawler-html/sanguo.md")
print(res)

>>> <Response [200]>
```

## Response对象的常用属性

- Python是一门面向对象编程的语言，而在爬虫中，理解数据是什么对象是非常、特别、以及极其重要的一件事。因为只有知道了数据是什么对象，我们才知道对象有什么属性和方法可供我们操作。
- 所以，我们现在来打印看看刚刚用`requests.get()`获取到的数据是什么。

```python
import requests
res = requests.get("https://localprod.pandateacher.com/python-manuscript/crawler-html/sanguo.md")
print(type(res))

>>> <class 'requests.models.Response'>
```

```python
# res是一个对象，属于requests.models.Response类。
```

- `Response`对象的常用属性：

|          属性          |               作用               |
| :--------------------: | :------------------------------: |
| `response.status_code` |         检查请求是否成功         |
|   `response.content`   | 把`response`对象转换为二进制数据 |
|    `response.text`     | 把`response`对象转换为字符串数据 |
|  `response.encoding`   |     定义`response`对象的编码     |

- **首先是`response.status_code`**

```python
import requests
res = requests.get("https://localprod.pandateacher.com/python-manuscript/crawler-html/sanguo.md")
print(res.status_code)

>>> 200
```

```python
#print(res.status_code)是在打印res的响应状态码，它可以用来检查我们的requests请求是否得到了成功的响应。我们看到终端结果显示了200，这个数字代表服务器同意了请求，并返回了数据给我们。
```

- 除了`200`，我们还可能收到其他的状态码。
- 下面有一个表格，供你参考不同的状态码代表什么，但不需要记住它们，在遇到问题的时候查询就好。
- **常见响应状态码解释**

| 响应状态码 |    说明    | 举例 |      说明      |
| :--------: | :--------: | :--: | :------------: |
|   `1xx`    |  请求收到  | 100  |  继续提出请求  |
|   `2xx`    |  请求成功  | 200  |      成功      |
|   `3xx`    |   重定向   | 305  | 应使用代理访问 |
|   `4xx`    | 客户端错误 | 403  |    禁止访问    |
|   `5xx`    | 服务器错误 | 503  |   服务不可用   |

- `response.status_code`是一个很常用的属性，在我们之后的爬虫代码中也将多次出现。
- **接着是`response.content`**
- 它能把Response对象的内容以二进制数据的形式返回，适用于图片、音频、视频的下载

```python
#举例说明
import requests
res = requests.get("https://res.pandateacher.com/2018-12-18-10-43-07.png")
#发出请求，并把返回的结果放在变量res中
pic = res.content
#把Reponse对象的内容以二进制数据的形式返回
photo = open("C:\\Users\\29678\\Desktop\\test\\ppt.jpg","wb")
#新建了一个文件ppt.jpg.
#图片内容需要以二进制wb读写。你在学习open()函数时接触过它。
photo.write(pic)
#获取pic的二进制内容
photo.close()
#关闭文件
```

- 这样，我们的图片就下载成功啦
- **继续看`response.text`**
- 这个属性可以把`Response`对象的内容以字符串的形式返回，适用于文字、网页源代码的下载。

```python
#举个实例：
import requests
res = requests.get("https://localprod.pandateacher.com/python-manuscript/crawler-html/sanguo.md")
nobel = res.text
#把Response对象的内容以字符串的形式返回
print(nobel[:100])
#现在，可以打印小说了，但考虑到整章太长，只输出100字看看就好。

>>> ## 三国演义

	作者：罗贯中

	### 第一回 宴桃园豪杰三结义 斩黄巾英雄首立功

	> 滚滚长江东逝水，浪花淘尽英雄。
	> 是非成败转头空。
	> 青山依旧在，几度夕阳红。　　
	>	
```

- 在此之后，我们就可以用通过读写文件把小说保存到本地了。

```python
#实现把小说保存到本地
import requests
res = requests.get("https://localprod.pandateacher.com/python-manuscript/crawler-html/sanguo.md")
novel = res.text
f = open("C:\\Users\\29678\\Desktop\\test\\《三国演义》.txt","a+")
f.write(novel)
f.close()
```

- **最后一个属性：`response.encoding`**
- 它能帮我们定义`Response`对象的编码。

```python
#举个实例：
import requests
res = requests.get("https://localprod.pandateacher.com/python-manuscript/crawler-html/sanguo.md")
res.encoding = "gbk"
novel = res.text
print(novel[:50])

>>> ## 涓夊浗婕斾箟

	浣滆�咃細缃楄疮涓�

	### 绗�涓�鍥� 瀹存�冨洯璞�鏉颁笁缁撲
```

- 为什么会出现一段乱码呢？
- 首先，目标数据本身有它的编码类型，这个《三国演义》URL中的数据类型是'utf-8'（因为这个网页是老师写的，所以我知道）。而用`requests.get()`发送请求后，我们得到一个`Response`对象，其中，`requests`模块会对数据的编码类型做出自己的判断。
- 但是，第5行的代码不管原来的判断是什么，直接定义了`Response`对象的编码类型是'gbk'。这样一来，跟数据本身的编码'utf-8'就不一致了，所以打印出来，就是一堆乱码。
- 如果我们把`gbk`换成`utf-8`，打印出来就没问题了：

```python
import requests
res = requests.get("https://localprod.pandateacher.com/python-manuscript/crawler-html/sanguo.md")
res.encoding = "UTF-8"
novel = res.text
print(novel[:50])

>>> ## 三国演义

	作者：罗贯中

	### 第一回 宴桃园豪杰三结义 斩黄巾英雄首立功
```

- 这只是个示范，是为了让大家理解`res.encoding`的意义，也就是它能定义`Response`对象的编码类型。
- 那在真实的情况中，我们该在什么时候用`res.encoding`呢？
- 首先，目标数据本身是什么编码是未知的。用`requests.get()`发送请求后，我们会取得一个`Response`对象，其中，`requests`库会对数据的编码类型做出自己的判断。但是！这个判断有可能准确，也可能不准确。
- 如果它判断准确的话，打印出来的`response.text`的内容就是正常的、没有乱码的，那就用不到`res.encoding`；如果判断不准确，就会出现一堆乱码，那就可以去查看目标数据的编码，然后再用`res.encoding`把编码定义成和目标数据一致的类型即可。
- 总的来说，就是遇上文本的乱码问题，才考虑用`res.encoding`。

# HTML基础

- 如果把HTML的学习依序分为三个层次的话，应该是读懂、修改、编写。
- 【读懂】：只有读懂了HTML，我们才能看得懂网页结构，才有可能运用Python的其他模块去解析数据和提取数据。所以想写爬虫程序的话，一定要先学好HTML基础。
- 修改】：在读懂HTML文档的基础上，学会修改HTML代码，是可以做些有趣的事情的。
- 【编写】：如果达到了这个水平，那就可以去应聘前端工程师了，这是专业的程序员水平了。
- 现在，只需要达到前两行个——读懂HTML、能够修改HTML文档即可。

## HTML是什么

- HTML（Hyper Text Markup Language）是用来描述网页的一种语言，也叫超文本标记语言 。
- 打个比方就更好理解了，HTML之于网页就好比建筑图纸之于建筑。
- 建筑图纸是建筑师设计房子时使用的语言，工匠会根据图纸内容，修建出它所描述的房子。
- 而HTML文档就是前端工程师设计网页时使用的语言，浏览器会根据HTML文档的描述，解析出它所描述的网页。
- 在之前，就讲到向浏览器中输入某个网址后，浏览器会向服务器发出请求，然后服务器就会作出响应。其实，服务器返回给浏览器的这个结果就是HTML代码了。
- 而紧接着，浏览器会根据这个HTML代码，解析成我们所能看见的漂亮的网页。

### 查看网页的HTML代码

- 【注：下面的示范，我用的是谷歌浏览器（`Chorme`）进行演示。】
- 先打开一个网址，在网页任意地方点击鼠标右键，然后点击“显示网页源代码”。（Windows系统的电脑还可以使用快捷键ctrl+u来查看网页源代码）

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\网页源代码1.png)

- 你会看到，浏览器弹出了一个新的标签页：

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\网页源代码2.png)

- 没错，这就是HTML源代码了。
- 样查看的好处是，整个网页的源代码都完整地呈现在你面前。坏处是，在大部分情况下，它都会经过压缩，导致结构不够清晰，你不太容易懂每行代码的含义。而且，源代码和网页分开在两个页面展示。
- 所以更多时候，我们会用这样一种方法：
- 在网页的空白处点击右键，然后选择“检查”（快捷方式是ctrl+shift+i）。

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\检查.png)

- 接着，你会看到一个新的界面——开发者工具栏：

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\检查1.png)

- 上图右边部分就是网页的HTML代码。
- 将鼠标放在HTML源代码上，你会发现，左边网页上有一些内容会被标亮。这其实就是这行代码所描述的网页内容，它们一左一右，相互对应。

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\检查2.png)

### HTML的层级

- 尽管在HTML里面有一些陌生的符号和文字，但不要担心噢，HTML很好入门。
- 因为，万事万物都是有条理的。
- 先以房子为例：房子由不同楼层所组成，每一层中，都会包含一些房间，一个房间还可能划分为几个更小的房间，每个房间又是由门、窗、墙壁、地板等等构建组成的。
- 所以HTML源代码之于网页，就像建筑图纸之于房子，会有鲜明的层级结构，以及互相对应的关系。
- 仔细观看自己网页的源代码：HTML源代码中有一些小三角形，每一个三角形都可以展开或合上。尖角向下代表展开，向右代表合上了，这就是HTML的层级关系。
- 这每一个可以展开和合上的小三角形里包含的内容，都是一个层级，它很像电脑中一层一层的文件夹。

## HTML的组成

### 标签和元素

之后再补上，现在没时间；{2022.0404.11.37}

———————————————————————————

## 获取网页源代码数据

- 需要爬取的网址：URL：https://localprod.pandateacher.com/python-manuscript/crawler-html/spider-men5.0.html

```python
import requests
ht = requests.get("https://localprod.pandateacher.com/python-manuscript/crawler-html/spider-men5.0.html")
#获取网页源代码，得到的res是response对象
print(ht.status_code)
#检查请求是否正确响应
print(ht.text)
#打印网页源代码的文本

>>> 200
<!DOCTYPE html>
<html>
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <title>这个书苑不太冷5.0</title>
        <style>
        a {
            text-decoration: none;
        }

        body {
            margin: 0;
            width:100%;
            height: 100%;
        }

        #header {
            background-color:#0c1f27;
            color:#20b2aa;
            text-align:center;
            padding:15px;
        }
 .......
{太多了就不一一展示了}
```

- 接下来，就把请求到的HTML源代码保存到本地：

```python
import requests
ht = requests.get("https://localprod.pandateacher.com/python-manuscript/crawler-html/spider-men5.0.html")
print(ht.status_code)
file = open("C:\\Users\\29678\\Desktop\\test\\book.txt","w",encoding = "utf-8")
file.writer(ht.text)
file.close()

>>> 200
```

- 如何做一个本地的网页：
- 把刚才请求到的HTML源文件后缀名改为`html`后，它就是一个保存在你本地中的网页了。
- `.html`这种文件格式，可以用浏览器打开。这就好比`MP3`文件用音乐播放器打开，`txt`用记事本打开。
- 你现在应该更明白浏览器的工作原理了，简单来说：浏览器从服务器上接收一个HTML文档，然后拿去做解析，最后呈现给你。因此，它也可以把你电脑的HTML文档解析成漂亮的网页。

# `BeautifulSoup`模块

## `BeautifulSoup`是什么

- 使用`BeautifulSoup`解析和提取网页中的数据。

- 【**解析数据**】是什么意思呢？
- 平时使用浏览器上网，浏览器会把服务器返回来的HTML源代码翻译为我们能看懂的样子，之后我们才能在网页上做各种操作。
- 而在爬虫中，也要使用能读懂`html`的工具，才能提取到想要的数据。

- 这就是解析数据。
- 【**提取数据**】是指把我们需要的数据从众多数据中挑选出来。
- **【小提醒】**：解析与提取数据在爬虫中，既是一个重点，也是难点。因此学习这两部分会比较吃力；希望做好下苦工的心里准备

## `BeautifulSoup`怎么用

- 不过首先需要安装`beautifulsoup4`模块

### 解析数据

- `BeautifulSoup`解析数据的用法很简单

|                      `bs`对象                      |
| :------------------------------------------------: |
| `bs`对象 = `BeautifulSoup`(要解析的文本，"解析器") |

- 在括号中，要输入两个参数，第0个参数是要被解析的文本，注意了，它必须必须必须是字符串。
- 括号中的第1个参数用来标识解析器，我们要用的是一个Python内置库：`html.parser`。（它不是唯一的解析器，但是比较简单的）。
- 我们看看具体的用法。仍然以网站这个书苑不太冷为例（url：https://localprod.pandateacher.com/python-manuscript/crawler-html/spider-men5.0.html），假设我们想爬取网页中的书籍类型、书名、链接、和书籍介绍。
- 根据之前所学的`requests.get()`，我们可以先获取到一个Response对象，并确认自己获取成功：

```python
import requests #调用requests库
res = requests.get('https://localprod.pandateacher.com/python-manuscript/crawler-html/spider-men5.0.html') 
#获取网页源代码，得到的res是response对象
print(res.status_code) #检查请求是否正确响应
html = res.text #把res的内容以字符串的形式返回
print(html)#打印html
```

- 接下来就轮到`BeautifulSoup`登场解析数据了。

```python
import requests
from bs4 import BeautifulSoup
#引入BS库，上面的bs4就是beautifulsoup4
res = requests.get("https://localprod.pandateacher.com/python-manuscript/crawler-html/spider-men5.0.html")
soup = BeautifulSoup(res.text,"html.parser")
# 把网页解析为BeautifulSoup对象
print(type(soup))
#查看soup的类型
print(soup)
#打印soup

>>> <class 'bs4.BeautifulSoup'>
	<!DOCTYPE html>
    {.......}
    {都是网页源代码就不一一复制下来了}
```

- 看看运行结果，`soup`的数据类型是`<class 'bs4.BeautifulSoup'>`，说明`soup`是一个`BeautifulSoup`对象。
- 打印的`soup`,它是我们所请求网页的完整HTML源代码。我们所要提取的书名、链接、书籍内容这些数据都在这里面。
- 也许会发现，打印`soup`出来的源代码和我们之前使用`response.text`打印出来的源代码是完全一样的。
- 也就是说，我们好不容易用`BeautifulSoup`写了一些代码来解析数据，但解析出的结果，竟然和没解析之前一样。
- 其实事情是这样的：虽然`response.text`和`soup`打印出的内容表面上看长得一模一样，却有着不同的内心，它们属于不同的类：`<class 'str'>` 与`<class 'bs4.BeautifulSoup'>`。前者是字符串，后者是已经被解析过的`BeautifulSoup`对象。之所以打印出来的是一样的文本，是因为`BeautifulSoup`对象在直接打印它的时候会调用该对象内的**`str`**方法，所以直接打印 `bs` 对象显示字符串是**`str`**的返回结果。
- 我们之后还会用`BeautifulSoup`库来提取数据，如果这不是一个`BeautifulSoup`对象，我们是没法调用相关的属性和方法的，所以，我们刚才写的代码是非常有用的，并不是重复劳动。
- 因此学会使用`BeautifulSoup`去解析数据：

```python
from bs4 import BeautifulSoup
soup = BeautifulSoup(字符串,'html.parser')
```

### 提取数据

- 仍然使用`BeautifulSoup`来提取数据。

| `BeautifulSoup`中提取数据的两大知识点： |
| :-------------------------------------: |
|         `find()`与`find_all()`          |
|                `Tag`对象                |

- 先看`find()`与`find_all()`。
- `find()`与`find_all()`是`BeautifulSoup`对象的两个方法，它们可以匹配`html`的标签和属性，把`BeautifulSoup`对象里符合要求的数据都提取出来。
- 俩的用法基本是一样的，区别在于，`find()`只提取首个满足要求的数据，而`find_all()`提取出的是所有满足要求的数据。
- `find()`与`find_all()`的用法：

|     方法     |          作用          |                   用法                   |                    示例                    |
| :----------: | :--------------------: | :--------------------------------------: | :----------------------------------------: |
|   `find()`   | 提取满足要求的首个数据 |   `BeautifulSoup对象.find(标签，属性)`   |   `soup.find` `("div",` `class_="book")`   |
| `find_all()` | 提取满足要求的所有数据 | `BeautifulSoup对象.find_all(标签，属性)` | `soup.find_all` `("div",` `class_="book")` |

- 想强调一下上表格中它们用法中的两个要点：
- 首先，请看举例中括号里的`class_`，这里有一个下划线，是为了和python语法中的类 `class`区分，避免程序冲突。当然，除了用`class`属性去匹配，还可以使用其它属性，比如`style`属性等。
- 其次，括号中的参数：标签和属性可以任选其一，也可以两个一起使用，这取决于我们要在网页中提取的内容。
- 如果只用其中一个参数就可以准确定位的话，就只用一个参数检索。如果需要标签和属性同时满足的情况下才能准确定位到我们想找的内容，那就两个参数一起使用。
- 看两个例子你就清楚了。以这个网页为例（URL: https://localprod.pandateacher.com/python-manuscript/crawler-html/spder-men0.0.html）：

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\提取数据.png)

- 在网页的HTML代码中，有三个`<div>`元素，用`find()`可以提取出首个元素，而`find_all()`可以提取出全部。

```python
import requests
from bs4 import BeautifulSoup
url = 'https://localprod.pandateacher.com/python-manuscript/crawler-html/spder-men0.0.html'
res = requests.get (url)
print(res.status_code)
soup = BeautifulSoup(res.text,'html.parser')
item = soup.find('div') #使用find()方法提取首个<div>元素，并放到变量item里。
print(type(item)) #打印item的数据类型
print(item)       #打印item 

>>> 200
	<class 'bs4.element.Tag'>
	<div>大家好，我是一个块</div>
```

- 看，运行结果正是首个`<div>`元素吧！我们还打印了它的数据类型：`<class 'bs4.element.Tag'>`，说明这是一个`Tag`类对象。
- 再来试试`find_all()`;它可以提取出网页中的全部三个`<div>`元素。

```python
import requests
from bs4 import BeautifulSoup
url = 'https://localprod.pandateacher.com/python-manuscript/crawler-html/spder-men0.0.html'
res = requests.get (url)
print(res.status_code)
soup = BeautifulSoup(res.text,'html.parser')
item = soup.find_all('div') #使用find()方法提取首个<div>元素，并放到变量item里。
print(type(item)) #打印item的数据类型
print(item)       #打印item 

>>> 200
	<class 'bs4.element.ResultSet'>
	[<div>大家好，我是一个块</div>, <div>我也是一个块</div>, <div>我还是一个块</div>]
```

- 运行结果是那三个`<div>`元素，它们一起组成了一个列表结构。打印`items`的类型，显示的是`<class 'bs4.element.ResultSet'>`，是一个`ResultSet`类的对象。其实是`Tag`对象以列表结构储存了起来，可以把它当做列表来处理。
- 再做一个小练习：
- 仍然以网站这个书苑不太冷为例：（url：https://localprod.pandateacher.com/python-manuscript/crawler-html/spider-men5.0.html），目标是爬取网页中的三本书的书名、链接、和书籍介绍。

```python
import requests # 调用requests库
from bs4 import BeautifulSoup # 调用BeautifulSoup库
res = requests.get('https://localprod.pandateacher.com/python-manuscript/crawler-html/spider-men5.0.html')# 返回一个Response对象，赋值给res
html = res.text# 把Response对象的内容以字符串的形式返回
soup = BeautifulSoup( html,'html.parser') # 把网页解析为BeautifulSoup对象
items = soup.find_all(class_='books') # 通过匹配标签和属性提取我们想要的数据
print(items) # 打印items
print(type(items)) #打印items的数据类型

>>> [<div class="books">
<h2><a name="type1">科幻小说</a></h2>
<a class="title" href="https://book.douban.com/subject/27077140/">《奇点遗民》</a>
<p class="info">本书精选收录了刘宇昆的科幻佳作共22篇。《奇点遗民》融入了科幻艺术吸引人的几大元素：数字化生命、影像化记忆、人工智能、外星访客……刘宇昆的独特之处在于，他写的不是科幻探险或英雄奇幻，而是数据时代里每个人的生活和情感变化。透过这本书，我们看到的不仅是未来还有当下。</p>
<img class="img" src="./spider-men5.0_files/s29492583.jpg"/>
<br>
<br>
<hr size="1"/>
</br></br></div>, <div class="books">
<h2><a name="type2">人文读物</a></h2>
<a class="title" href="https://book.douban.com/subject/26943161/">《未来简史》</a>
<p class="info">未来，人类将面临着三大问题：生物本身就是算法，生命是不断处理数据的过程；意识与智能的分离；拥有大数据积累的外部环境将比我们自己更了解自己。如何看待这三大问题，以及如何采取应对措施，将直接影响着人类未来的发展。</p>
<img class="img" src="./spider-men5.0_files/s29287103.jpg"/>
<br>
<br/>
<hr size="1"/>
</br></div>, <div class="books">
<h2><a name="type3">技术参考</a></h2>
<a class="title" href="https://book.douban.com/subject/25779298/">《利用Python进行数据分析》</a>
<p class="info">本书含有大量的实践案例，你将学会如何利用各种Python库（包括NumPy、pandas、matplotlib以及IPython等）高效地解决各式各样的数据分析问题。由于作者Wes McKinney是pandas库的主要作者，所以本书也可以作为利用Python实现数据密集型应用的科学计算实践指南。本书适合刚刚接触Python的分析人员以及刚刚接触科学计算的Python程序员。</p>
<img class="img" src="./spider-men5.0_files/s27275372.jpg"/>
<br/>
<br/>
<hr size="1"/>
</div>]
<class 'bs4.element.ResultSet'>
```

- 现在，三本书的全部信息都被我们提取出来了。它的数据类型是`<class 'bs4.element.ResultSet'>`， 前面说过可以把它当做列表`list`来看待。
- 不过，列表并不是我们最终想要的东西，我们想要的是列表中的值，所以要想办法提取出列表中的每一个值。
- 用`for`循环遍历列表，就可以把这三个`<div>`元素取出来了。

```python
import requests # 调用requests库
from bs4 import BeautifulSoup # 调用BeautifulSoup库
res = requests.get('https://localprod.pandateacher.com/python-manuscript/crawler-html/spider-men5.0.html')# 返回一个Response对象，赋值给res
html= res.text# 把Response对象的内容以字符串的形式返回
soup = BeautifulSoup( html,'html.parser') # 把网页解析为BeautifulSoup对象
items = soup.find_all(class_='books') # 通过定位标签和属性提取我们想要的数据
for item in items:
    print('想找的数据都包含在这里了：\n',item) # 打印item

>>> 想找的数据都包含在这里了：
 <div class="books">
<h2><a name="type1">科幻小说</a></h2>
<a class="title" href="https://book.douban.com/subject/27077140/">《奇点遗民》</a>
<p class="info">本书精选收录了刘宇昆的科幻佳作共22篇。《奇点遗民》融入了科幻艺术吸引人的几大元素：数字化生命、影像化记忆、人工智能、外星访客……刘宇昆的独特之处在于，他写的不是科幻探险或英雄奇幻，而是数据时代里每个人的生活和情感变化。透过这本书，我们看到的不仅是未来还有当下。</p>
<img class="img" src="./spider-men5.0_files/s29492583.jpg"/>
<br>
<br>
<hr size="1"/>
</br></br></div>
	想找的数据都包含在这里了：
 <div class="books">
<h2><a name="type2">人文读物</a></h2>
<a class="title" href="https://book.douban.com/subject/26943161/">《未来简史》</a>
<p class="info">未来，人类将面临着三大问题：生物本身就是算法，生命是不断处理数据的过程；意识与智能的分离；拥有大数据积累的外部环境将比我们自己更了解自己。如何看待这三大问题，以及如何采取应对措施，将直接影响着人类未来的发展。</p>
<img class="img" src="./spider-men5.0_files/s29287103.jpg"/>
<br>
<br/>
<hr size="1"/>
</br></div>
	想找的数据都包含在这里了：
 <div class="books">
<h2><a name="type3">技术参考</a></h2>
<a class="title" href="https://book.douban.com/subject/25779298/">《利用Python进行数据分析》</a>
<p class="info">本书含有大量的实践案例，你将学会如何利用各种Python库（包括NumPy、pandas、matplotlib以及IPython等）高效地解决各式各样的数据分析问题。由于作者Wes McKinney是pandas库的主要作者，所以本书也可以作为利用Python实现数据密集型应用的科学计算实践指南。本书适合刚刚接触Python的分析人员以及刚刚接触科学计算的Python程序员。</p>
<img class="img" src="./spider-men5.0_files/s27275372.jpg"/>
<br/>
<br/>
<hr size="1"/>
</div>
```

- 结果正是那三个`<div>`元素。
- 其实到这里，`find()`和`find_all()`的用法讲了，练习也做了，但是，我们现在打印出来的东西还不是目标数据，里面含着HTML标签，所以下面，我们要进入到提取数据中的另一个知识点——`Tag`对象。
- 咱们还以上面的代码为例，我们现在拿到的是一个个包含html标签的数据，还没达成目标。
- 这个时候，我们一般会选择用`type()`函数查看一下数据类型，因为Python是一门面向对象编程的语言，只有知道是什么对象，才能调用相关的对象属性和方法。

```python
import requests # 调用requests库
from bs4 import BeautifulSoup # 调用BeautifulSoup库
res = requests.get('https://localprod.pandateacher.com/python-manuscript/crawler-html/spider-men5.0.html') # 返回一个response对象，赋值给res
html = res.text# 把res的内容以字符串的形式返回
soup = BeautifulSoup( html,'html.parser') # 把网页解析为BeautifulSoup对象
items = soup.find_all(class_='books') # 通过定位标签和属性提取我们想要的数据
for item in items:
    print(type(item))

>>> <class 'bs4.element.Tag'>
	<class 'bs4.element.Tag'>
	<class 'bs4.element.Tag'>
```

- 我们看到它们的数据类型是`<class 'bs4.element.Tag'>`，是`Tag`对象，不知道你是否还记得，这与`find()`提取出的数据类型是一样的。
- 好，既然知道了是`Tag`对象，下一步，就是看看`Tag`类对象的常用属性和方法了。

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\Tag对象的三种常用属性与方法.png)

- 上图是`Tag`对象的3种用法，咱们一个一个来讲。
- 首先，`Tag`对象可以使用`find()`与`find_all()`来继续检索。
- 回到我们刚刚写的代码：即爬取这个书苑不太冷网站中每本书的类型、链接、标题和简介，我们刚刚拿到的分别是三本书的内容，即三个`Tag`对象。现在，先把首个`Tag`对象展示在下面，方便我们阅读：

```python
<div class="books">
    <h2><a name="type1">科幻小说</a></h2>
    <a href="https://book.douban.com/subject/27077140/" class="title">《奇点遗民》</a>
    <p class="info">本书精选收录了刘宇昆的科幻佳作共22篇。《奇点遗民》融入了科幻艺术吸引人的几大元素：数字化生命、影像化记忆、人工智能、外星访客……刘宇昆的独特之处在于，他写的不是科幻探险或英雄奇幻，而是数据时代里每个人的生活和情感变化。透过这本书，我们看到的不仅是未来还有当下。
    </p> 
    <img class="img" src="./spider-men5.0_files/s29492583.jpg">
    <br>
    <br>
    <hr size="1">
</div>
```

- 看第2行：书籍的类型在这里面；第3行：我们要取的链接和书名在里面；第4行：书籍的简介在里面。因为是只取首个数据，这次用`find()`就好。

```python
import requests # 调用requests库
from bs4 import BeautifulSoup # 调用BeautifulSoup库
res = requests.get('https://localprod.pandateacher.com/python-manuscript/crawler-html/spider-men5.0.html')
# 返回一个response对象，赋值给res
html = res.text
# 把res的内容以字符串的形式返回
soup = BeautifulSoup( html,'html.parser') 
# 把网页解析为BeautifulSoup对象
items = soup.find_all(class_='books') # 通过定位标签和属性提取我们想要的数据
for item in items:
    kind = item.find('h2') # 在列表中的每个元素里，匹配标签<h2>提取出数据
    title = item.find(class_='title') #在列表中的每个元素里，匹配属性class_='title'提取出数据
    brief = item.find(class_='info') #在列表中的每个元素里，匹配属性class_='info'提取出数据
    print(kind,'\n',title,'\n',brief) # 打印提取出的数据
    print(type(kind),type(title),type(brief)) # 打印提取出的数据类型
    
>>> <h2><a name="type1">科幻小说</a></h2> 
 <a class="title" href="https://book.douban.com/subject/27077140/">《奇点遗民》</a> 
 <p class="info">本书精选收录了刘宇昆的科幻佳作共22篇。《奇点遗民》融入了科幻艺术吸引人的几大元素：数字化生命、影像化记忆、人工智能、外星访客……刘宇昆的独特之处在于，他写的不是科幻探险或英雄奇幻，而是数据时代里每个人的生活和情感变化。透过这本书，我们看到的不仅是未来还有当下。</p>
<class 'bs4.element.Tag'> <class 'bs4.element.Tag'> <class 'bs4.element.Tag'>
<h2><a name="type2">人文读物</a></h2> 
 <a class="title" href="https://book.douban.com/subject/26943161/">《未来简史》</a> 
 <p class="info">未来，人类将面临着三大问题：生物本身就是算法，生命是不断处理数据的过程；意识与智能的分离；拥有大数据积累的外部环境将比我们自己更了解自己。如何看待这三大问题，以及如何采取应对措施，将直接影响着人类未来的发展。</p>
<class 'bs4.element.Tag'> <class 'bs4.element.Tag'> <class 'bs4.element.Tag'>
<h2><a name="type3">技术参考</a></h2> 
 <a class="title" href="https://book.douban.com/subject/25779298/">《利用Python进行数据分析》</a> 
 <p class="info">本书含有大量的实践案例，你将学会如何利用各种Python库（包括NumPy、pandas、matplotlib以及IPython等）高效地解决各式各样的数据分析问题。由于作者Wes McKinney是pandas库的主要作者，所以本书也可以作为利用Python实现数据密集型应用的科学计算实践指南。本书适合刚刚接触Python的分析人员以及刚刚接触科学计算的Python程序员。</p>
<class 'bs4.element.Tag'> <class 'bs4.element.Tag'> <class 'bs4.element.Tag'>
```

- 除了我们拿到的数据之外；运行结果的数据类型，又是三个`<class 'bs4.element.Tag'>`，用`find()`提取出来的数据类型和刚才一样，还是`Tag`对象。接下来要做的，就是把`Tag`对象中的文本内容提出来。
- 这时，可以用到`Tag`对象的另外两种属性——`Tag.text`，和`Tag['属性名']`。
- 我们用`Tag.text`提出`Tag`对象中的文字，用`Tag['href']`提取出URL。

```python
import requests # 调用requests库
from bs4 import BeautifulSoup # 调用BeautifulSoup库
res =requests.get('https://localprod.pandateacher.com/python-manuscript/crawler-html/spider-men5.0.html')
# 返回一个response对象，赋值给res
html=res.text
# 把res解析为字符串
soup = BeautifulSoup( html,'html.parser')
# 把网页解析为BeautifulSoup对象
items = soup.find_all(class_='books')   # 通过匹配属性class='books'提取出我们想要的元素
for item in items:  
    wenben = item.text
    print(wenben)
  
>>> 科幻小说
《奇点遗民》
本书精选收录了刘宇昆的科幻佳作共22篇。《奇点遗民》融入了科幻艺术吸引人的几大元素：数字化生命、影像化记忆、人工智能、外星访客……刘宇昆的独特之处在于，他写的不是科幻探险或英雄奇幻，而是数据时代里每个人的生活和情感变化。透过这本书，我们看到的不仅是未来还有当下。

人文读物
《未来简史》
未来，人类将面临着三大问题：生物本身就是算法，生命是不断处理数据的过程；意识与智能的分离；拥有大数据积累的外部环境将比我们自己更了解自己。如何看待这三大问题，以及如何采取应对措施，将直接影响着人类未来的发展。

技术参考
《利用Python进行数据分析》
本书含有大量的实践案例，你将学会如何利用各种Python库（包括NumPy、pandas、matplotlib以及IPython等）高效地解决各式各样的数据分析问题。由于作者Wes McKinney是pandas库的主要作者，所以本书也可以作为利用Python实现数据密集型应用的科学计算实践指南。本书适合刚刚接触Python的分析人员以及刚刚接触科学计算的Python程序员。

```

- 看看终端，拿出来啦~(≧▽≦)/~此处应该有掌声，到这里，我们终于成功解析、提取到了所有的数据。
- 不过呢，每个网页都有自己的结构，我们写爬虫程序，还是得坚持从实际出发，具体问题具体分析哈。
- 爬虫前三步的总过程：

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\爬虫前三步的总过程.png)

# 重新分析过程

## `Network`

- 我们先去看看Network的页面。在你刚才打开的QQ音乐页面，调用“检查”（`ctrl+shift+i`）工具，然后点击`Network`。
- `Network`的功能是：记录在当前页面上发生的所有请求。现在看上去好像空空如也的样子，这是因为`Network`记录的是实时网络请求。现在网页都已经加载完成，所以不会有东西。
- 我们点击一下刷新，浏览器会重新访问网络，这样就会有记录。如下图：

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\netweek-16492254423791.png)

- 在图最下面，它告诉我们：此处共有38个请求，21.1KB的流量，1.2MB种子资源,耗时11.06s完成。
- 这个，正是我们的浏览器每时每刻工作的真相：它总是在向服务器，发起各式各样的请求。当这些请求完成，它们会一起组成我们在`Elements`中看到的网页源代码。
- 为什么我们刚才没办法拿到歌曲清单呢？答，这是因为我们刚刚写的代码，只是模拟了这38个请求中的一个（准确来说，就是第0个请求），而这个请求里并不包含歌曲清单。
- 在请挪动鼠标，找到这个页面的第0个请求：`search.html`，然后点击它，如下图，我们来查看它的`Response`（官方翻译叫“响应”，你可以理解为服务器对浏览器这个请求的回应内容，即请求的结果）。

![]()![响应](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\响应.png)

- 其实，它就是我们刚刚用`requests.get()`获取到的网页源代码，它里面不包含歌曲清单。
- 一般来说，都是这种第0个请求先启动了，其他的请求才会关联启动，一点点地将网页给填充起来。做一个比喻，第0个请求就好比是人的骨架，确定了这个网页的结构。在此之后，众多的请求接连涌入，作为人的血脉经络。如此，人就变好看。
- 当然啦，也有一些网页，直接把所有的关键信息都放在第0个请求里，尤其是一些比较老（或比较轻量）的网站，我们用`requests`和`BeautifulSoup`就能解决它们。比如我们体验过的“这个书苑不太冷”，比如你看过的“人人都是蜘蛛侠”博客，比如豆瓣。
- 总之，为了成功抓取到歌曲清单。我们得先找到，歌名藏在哪一个请求当中。再用`requests`库，去模拟这个请求。

## `Network`怎么用

- 想做这个，我们需要先去了解下`Network`面板怎么用。回头看我们之前给的图：

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\network面板-16492266198062.png)

- 0：表示启用【Network】监控
- 1：清空面板信息
- 2：ALL(查看全部)
- 3：XHR(仅查看XHR)
- 4：点击勾选框作用是"保留请求日志"

```python
#先讲解：0，1，4
红色的圆钮是启用Network监控（默认高亮打开），灰色圆圈是清空面板上的信息。右侧勾选框Preserve log{保留日志}，它的作用是“保留请求日志”。如果不点击这个，当发生页面跳转的时候，记录就会被清空。所以，我们在爬取一些会发生跳转的网页时，会点亮它。
#讲解：2，3
是对请求进行分类查看。我们最常用的是：ALL（查看全部）/XHR（仅查看XHR，我们等会重点讲它）/Doc（Document，第0个请求一般在这里），有时候也会看看：Img（仅查看图片）/Media（仅查看媒体文件）/Other（其他）。最后，JS和CSS，则是前端代码，负责发起请求和页面实现；Font是文字的字体；而理解WS和Manifest，需要网络编程的知识，倘若不是专门做这个，你不需要了解。
```

| `ALL`          | 查看全部                             |
| -------------- | ------------------------------------ |
| `XHR`          | 一种不借助刷新网页即可传输数据的对象 |
| `Doc`          | Document,第0个请求一般在这里         |
| `lmg`          | 仅查看图片                           |
| `Media`        | 仅查看媒体文件                       |
| `Other`        | 其他                                 |
| `JS和CSS`      | 前端代码，负责发起请求和页面实现     |
| `Font`         | 字体                                 |
| `WS和Manifest` | 网络编程相关知识，无需了解           |

## 什么是XHR？

- 在`Network`中，有一类非常重要的请求叫做`XHR`（当你把鼠标在XHR上悬停，你可以看到它的完整表述是XHR and Fetch）
- 我们平时使用浏览器上网的时候，经常有这样的情况：浏览器上方，它所访问的网址没变，但是网页里却新加了内容。
- 典型代表：如购物网站，下滑自动加载出更多商品。在线翻译网站，输入中文实时变英文。比如，正在使用的教学系统，每点击一次Enter就有新的内容弹出。
- 这个，叫做Ajax技术（技术本身和爬虫关系不大，在此不做展开，你可以通过搜索了解）。应用这种技术，好处是显而易见的——更新网页内容，而不用重新加载整个网页。又省流量又省时间的，何乐而不为。
- 如今，比较新潮的网站都在使用这种技术来实现数据传输。只剩下一些特别老，或是特别轻量的网站，还在用老办法——加载新的内容，必须要跳转一个新网址。
- 这种技术在工作的时候，会创建一个`XHR`（或是Fetch）对象，然后利用`XHR`对象来实现，服务器和浏览器之间传输数据。在这里，`XHR`和`Fetch`并没有本质区别，只是`Fetch`出现得比`XHR`更晚一些，所以对一些开发人员来说会更好用，但作用都是一样的。

## XHR怎么请求？

- 显而易见，对照前面的表单。我们的歌曲清单不在网页源代码里，而且也不是图片，不是媒体文件，自然只会是在`XHR`里。我们现在去找找看，点击`XHR`按钮。

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\HXR.png)

- 这个网页里一共有7个`XHR`或`Fetch`，我们要从里面找出带有歌单的那一个。
- 笨办法当然是一个一个实验，但聪明的办法是去尝试阅读它们的名字。比如你一眼就看到：`musics.fccg`……而且它最大，有10.7KB，我们来点击它。

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\XHR.png)

- 点击`Preview`，你能在里面发现我们想要的信息：歌名就藏在里面！只是有点难找，需要你一层一层展开：`data-song-list-0-name`）

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\歌名.png)

- 那如何把这些歌曲名拿到呢？这就需要我们去看看最左侧的`Headers`（常规），点击它。如下所示，它被分为三个板块。

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\常规.png)

- 点开它，你会看到：

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\常规1.png)

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\常规2.png)

- 看到了吗？`General`里的`Requests URL`就是我们应该去访问的链接。如果在浏览器中打开这个链接，你会看到一个让人绝望的结构：最外层是一个字典，然后里面又是字典，往里面又有列表和字典……
- 它就和你在`Response`里看到的一个样。还是放弃挣扎吧，回到原网址，直接用`Preview`来看就好。列表和字典在此都会有非常清晰的结构，层层展开。

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\预览.png)

- 如上，我们一层一层地点开，按照这样的顺序：`data-song-list-0-name`，看到：

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\歌名2.png)

- 曲名就在这里，它的键是`name`。理解这句话：这个`XHR`是一个字典，键`data`对应的值也是一个字典；在该字典里，键`song`对应的值也是一个字典；在该字典里，键`list`对应的值是一个列表；在该列表里，一共有10个元素；每一个元素都是一个字典；在每个字典里，键`name`的值，对应的是歌曲名。
- 此刻的你有了一个大胆的想法：利用`requests.get()`访问这个链接，把这个字典下载到本地。然后去一层一层地读取，拿到歌曲名。
- 到此，我们的代码可以写成这样，你可以尝试运行看看：

```python
# 引用requests库    
import requests
# 调用get方法，下载这个字典
res = requests.get('https://c.y.qq.com/soso/fcgi-bin/client_search_cp?ct=24&qqmusic_ver=1298&new_json=1&remoteplace=txt.yqq.song&searchid=60997426243444153&t=0&aggr=1&cr=1&catZhida=1&lossless=0&flag_qc=0&p=1&n=20&w=%E5%91%A8%E6%9D%B0%E4%BC%A6&g_tk=5381&loginUin=0&hostUin=0&format=json&inCharset=utf8&outCharset=utf-8&notice=0&platform=yqq.json&needNewCode=0')
# 把它打印出来
print(res.text)

>>> {"code":0,"data":{"keyword":"周杰伦","priority":0,"qc":[],"semantic":{"curnum":0,"curpage":0,"list":[],"totalnum":0},"song":{"curnum":0,"curpage":1,"list":[],"totalnum":0},"tab":0,"taglist":[],"totaltime":0,"zhida":null},"message":"","notice":"","subcode":0,"time":1649229006,"tips":""}
```

- 在这里，我们又遇到一个障碍：使用`res.text`取到的，是字符串。它不是我们想要的列表/字典，数据取不出来。

## `json`是什么？

- `json`是什么呢？粗暴地来解释，`json`是一种数据交换的**语法**。对我们来说，它只是一种规范数据传输的格式，形式有点像字典和列表的结合体。

```python
# 定义一个字典  
a = {'name':'吴枫'}
# 定义一张列表
b = [1,2,3,4]
# 定义一个json
c = {
        "forchange": 
        [
            { "name":"吴枫" , "gender":"male"}, 
            { "name":"酱酱" , "gender":"female"}, 
            { "name":"延君" , "gender":"male"},
        ]
    }
```

- 从它的组成上来看，有`花括弧`、`方括弧`，`冒号`和`逗号`，一种字典和列表相互嵌套的体系。
- 这种特殊的写法决定了，`json`能够有组织地存储信息。
- 我们在生活当中，总是在接触林林总总的数据。如果它们直接以堆砌的形式出现在你面前，你很难阅读它。比如：想象一个乱序排布的字典，一个堆满文件的电脑桌面，一本不分段落章节的小说……
- 数据需要被有规律地组织起来，我们才能去查找、阅读、分析、理解。比如：汉语字典应该按照拼音排序，文件应该按照一定规律放进不同的文件夹，小说要有章节目录——大标题、中标题、小标题。
- 可以发现，组织数据的方式也有规律，规律有三条：

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\组织数据的规律.png)

- 一般来说，这三条占得越多，数据的结构越清晰；占得越少，数据的结构越混沌。
- 生活如此，网络之间的数据传输也是如此。在之前，我们已经学习过`html`，它通过标签、属性来实现分层和对应。
- `json`则是另一种组织数据的格式，长得和Python中的列表/字典非常相像。它和`html`一样，常用来做网络数据传输。刚刚我们在`XHR`里查看到的列表/字典，严格来说其实它不是列表/字典，它是`json`。

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\josn.png)

- 或许你会有疑问：那直接写成列表/字典不就好了，为什么要把它表示成字符串？答案很简单，因为不是所有的编程语言都能读懂Python里的数据类型（如，列表/字典），但是所有的编程语言，都支持文本（比如在Python中，用字符串这种数据类型来表示文本）这种最朴素的数据类型。
- 如此，`json`数据才能实现，跨平台，跨语言工作。
- 而`json`和`XHR`之间的关系：`XHR`用于传输数据，它能传输很多种数据，`json`是被传输的一种数据格式。就是这样而已。
- 我们总是可以将`json`格式的数据，转换成正常的列表/字典，也可以将列表/字典，转换成`json`。

## `json`数据如何解析

- 当我们请求得到了`json`数据，应该如何读取呢？我们可以在`requests`库的官方文档中，找到答案。我们打开浏览器，搜索“requests 官方文档”，会来到这个界面：

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\requsts.png)

- 点开链接，进入文档，你会看到一个非常傲娇的作者。

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\傲娇.png)

- 使用浏览器的`ctrl+f`功能，在网页内搜索关键词`json`，能够非常快捷地找到这里：

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\json搜索.png)

- 点击进入，你将看到`requests`库处理`json`数据的方法。

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\json响应.png)

- 你看方法很简单，请求到数据之后，使用`json()`方法即可成功读取。接下来的操作，就和列表/字典相一致。

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\图.png)

- 用一个小案例来加深印象：

```python
# 引用requests库
import requests
# 调用get方法，下载这个字典
res_music = requests.get('https://c.y.qq.com/soso/fcgi-bin/client_search_cp?ct=24&qqmusic_ver=1298&new_json=1&remoteplace=txt.yqq.song&searchid=60997426243444153&t=0&aggr=1&cr=1&catZhida=1&lossless=0&flag_qc=0&p=1&n=20&w=%E5%91%A8%E6%9D%B0%E4%BC%A6&g_tk=5381&loginUin=0&hostUin=0&format=json&inCharset=utf8&outCharset=utf-8&notice=0&platform=yqq.json&needNewCode=0')
# 使用json()方法，将response对象，转为列表/字典
json_music = res_music.json()
# 打印json_music的数据类型
print(type(json_music))

>>> <class 'dict'>
```

# 带参数请求数据

## 什么是带参数请求数据

- 我不知道你有没有认真地观察过一个完整url的组成，如果没有，我们现在来试试看：
- 当你在豆瓣搜索“海边的卡夫卡”，它的网址会是这样：

```python
https://www.douban.com/search?q=%E6%B5%B7%E8%BE%B9%E7%9A%84%E5%8D%A1%E5%A4%AB%E5%8D%A1
```

- 当你在知乎搜索“宇宙大爆炸”，它的网址会是这样：

```python
https://www.zhihu.com/search?type=content&q=%E5%AE%87%E5%AE%99%E5%A4%A7%E7%88%86%E7%82%B8
```

- 现在，我要揭晓规律：
- 在上面，我们能看到每个`url`都由两部分组成。前半部分大多形如：https://xx.xx.xxx/xxx/xxx
- 后半部分，多形如：xx=xx&xx=xxx&xxxxx=xx&……
- 两部分使用`?`来连接。举例刚刚的豆瓣网址，前半部分就是：

```python
https://www.douban.com/search
```

- 后半部分则是：

```python
q=%E6%B5%B7%E8%BE%B9%E7%9A%84%E5%8D%A1%E5%A4%AB%E5%8D%A1
```

- 它们的中间使用了`?`来隔开。
- 这前半部分是我们所请求的地址，它告诉服务器，我想访问这里。而后半部分，就是我们的请求所附带的参数，它会告诉服务器，我们想要**什么样**的数据。
- 这参数的结构，会和字典很像，有键有值，键值用=连接；每组键值之间，使用&来连接。
- 而我们的请求所附带的参数是“海边的卡夫卡”：`q=%E6%B5%B7%E8%BE%B9%E7%9A%84%E5%8D%A1%E5%A4%AB%E5%8D%A1`(那段你看不懂的代码，它是“海边的卡夫卡”使用utf-8编码的结果)。
- 

## 如何带参数请求数据

# `Request Headers`

## 什么是`Request Headers`

- 就是这个：

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\请求标头.png)

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\请求标头1.png)

- 每一个请求，都会有一个`Request Headers`，我们把它称作请求头。它里面会有一些关于该请求的基本信息，比如：这个请求是从什么设备什么浏览器上发出？这个请求是从哪个页面跳转而来？
- 如上图，`user-agent`（中文：用户代理）会记录你电脑的信息和浏览器版本（如我的，就是windows10的64位操作系统，使用谷歌浏览器）。
- `origin`（中文：源头）和`referer`（中文：引用来源）则记录了这个请求，最初的起源是来自哪个页面。它们的区别是`referer`会比`origin`携带的信息更多些。
- 如果我们想告知服务器，我们不是爬虫，而是一个正常的浏览器，就要去修改`user-agent`。倘若不修改，那么这里的默认值就会是Python，会被服务器认出来。
- 有趣的是，像百度的爬虫，它的`user-agent`就会是`Baiduspider`，谷歌的也会是`Googlebot`……如是种种。
- 而对于爬取某些特定信息，也要求你注明请求的来源，即`origin`或`referer`的内容。比如我有试过，在爬取歌曲详情页里的歌词时，就需要注明这个信息，否则会拿不到歌词。你可以在写练习的时候进行尝试。

## 如何添加`Request Headers`

- Requests模块允许我们去修改Headers的值。点击它的官方文档，搜索“user-agent”，你会看到：

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\定制.png)

- 如上，只需要封装一个字典就好了。和写params非常相像。
- 而修改`origin`或`referer`也和此类似，一并作为字典写入headers就好。就像这样：

```python
import requests
url = 'https://c.y.qq.com/soso/fcgi-bin/client_search_cp'

headers = {
    'origin':'https://y.qq.com',
    # 请求来源，本案例中其实是不需要加这个参数的，只是为了演示
    'referer':'https://y.qq.com/n/yqq/song/004Z8Ihr0JIu5s.html',
    # 请求来源，携带的信息比“origin”更丰富，本案例中其实是不需要加这个参数的，只是为了演示
    'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36',
    # 标记了请求从什么设备，什么浏览器上发出
    }
# 伪装请求头
params = {
'ct':'24',
    #中间还有一堆，我就没有写在里面了
'needNewCode':'0'    
}
#将参数封装为字典
res_music = requests.get(url,headers=headers,params=params)
# 发起请求，填入请求头和参数
```

- 如果有一天，你真的需要爬取一万多条信息，将for循环执行成百上千次。
- 那么，你最好将自己的爬虫伪装成真实的浏览器（填写请求头）—— 因为在那种情况下，服务器很可能拒绝爬虫访问。甚至有的网站，一开始就不允许爬虫访问。如，知乎、猫眼电影。

# 存储数据

## 存储数据的方法

- 其实，常用的存储数据的方式有两种——存储成csv格式文件、存储成Excel文件（不是复制黏贴的那种）。
- 我猜想，此时你会想问“csv”是什么，和Excel文件有什么区别？
- 前面，我有讲到json是特殊的字符串。其实，csv也是一种字符串文件的格式，它组织数据的语法就是在字符串之间加分隔符——行与行之间是加换行符，同行字符之间是加逗号分隔。
- 它可以用任意的文本编辑器打开（如记事本），也可以用Excel打开，还可以通过Excel把文件另存为csv格式（因为Excel支持csv格式文件）。

```python
file=open('test.csv','a+')
#创建test.csv文件，以追加的读写模式
file.write('美国队长,钢铁侠,蜘蛛侠')
#写入test.csv文件
file.close()
#关闭文件
```

- 将我们刚刚写入的csv文件下载到本地电脑，再用记事本打开，你会看到：

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\记事本.png)

- 用Excel打开，则是这样的：

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\excel.png)

- 发现了吗？csv文件里的逗号可以充当分隔同行字符串的作用。
- 为什么要加分隔符？因为不加的话，数据都堆在一起，会显得杂乱无章，也不方便我们之后提取和查找。这也是一种让数据变得有规律的组织方式。
- 另外，用csv格式存储数据，读写比较方便，易于实现，文件也会比Excel文件小。但csv文件缺少Excel文件本身的很多功能，比如不能嵌入图像和图表，不能生成公式。
- 至于Excel文件，不用我多说你也知道就是电子表格。它有专门保存文件的格式，即xls和xlsx（Excel2003版本的文件格式是xls，Excel2007及之后的版本的文件格式就是xlsx）。

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\区别.png)

## 存储数据的基础知识

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\存储数据基础知识.png)

- 存储成csv格式文件和存储成Excel文件，这两种不同的存储方式需要引用的模块也是不同的。操作csv文件我们需要借助csv模块；操作Excel文件则需要借助openpyxl模块。

### 基础知识：`Excel`写入与读取

#### `Excel`文件写入

- 在开始讲Excel文件的写入与读取前，我们还得稍微了解一下Excel文档的基本概念（考验你对Excel有多了解的时候到了٩(๑❛ᴗ❛๑)۶）。
- 工作簿、工作表和单元格在Excel里分别是指什么？
- 一个Excel文档也称为一个工作簿（workbook），每个工作簿里可以有多个工作表（worksheet），当前打开的工作表又叫活动表。
- 每个工作表里有行和列，特定的行与列相交的方格称为单元格（cell）。比如第A列和第1行相交的方格我们可以直接表示为A1单元格。
- 清楚了Excel的基础概念，我们可以来说下openpyxl模块是怎么操作Excel文件的了。照例先说写入后说读取。
- 装好openpyxl模块后，首先要引用它，然后通过openpyxl.Workbook()函数就可以创建新的工作簿，代码如下:

```python
# 引用openpyxl     
import openpyxl 

# 利用openpyxl.Workbook()函数创建新的workbook（工作簿）对象，就是创建新的空的Excel文件。
wb = openpyxl.Workbook()
```

- 创建完新的工作簿后，还得获取工作表。不然程序会无所适从，不知道要把内容写入哪张工作表里。

```python
# wb.active就是获取这个工作簿的活动表，通常就是第一个工作表。
sheet = wb.active

# 可以用.title给工作表重命名。现在第一个工作表的名称就会由原来默认的“sheet1”改为"new title"。
sheet.title = 'new title'
```

- 添加完工作表，我们就能来操作单元格，往单元格里写入内容。

```python
# 把'漫威宇宙'赋值给第一个工作表的A1单元格，就是往A1的单元格中写入了'漫威宇宙'。
sheet['A1'] = '漫威宇宙' 
```

- 往单元格里写入内容只要定位到具体的单元格，如A1（根据Excel的坐标，A1代表第一列第一行相交的单元格），然后给这个单元格赋值即可。
- 如果我们想往工作表里写入一行内容的话，就得用到append函数。

```python
# 把我们想写入的一行内容写成列表，赋值给row。
row = ['美国队长','钢铁侠','蜘蛛侠']

# 用sheet.append()就能往表格里添加这一行文字。
sheet.append(row)
```

- 如果我们想要一次性写入的不止一行，而是多行内容，又该怎么办？

```python
# 先把要写入的多行内容写成列表，再放进大列表里，赋值给rows。
rows = [['美国队长','钢铁侠','蜘蛛侠'],['是','漫威','宇宙', '经典','人物']]

# 遍历rows，同时把遍历的内容添加到表格里，这样就实现了多行写入。
for i in rows:
    sheet.append(i)

# 打印rows
print(rows)
```

- 成功写入后，我们千万要记得保存这个Excel文件，不然就白写啦！

```python
# 保存新建的Excel文件，并命名为“Marvel.xlsx”
wb.save('Marvel.xlsx')
```

- 这样，Excel文件写入的代码我们就写好了
- 总体代码如下：

```python
import openpyxl

wb = openpyxl.Workbook()
sheet = wb.active
sheet.title = "new title"
sheet["A1"] = "漫威宇宙"
rows = [['美国队长','钢铁侠','蜘蛛侠'],['是','漫威','宇宙', '经典','人物']]
for i in rows:
    sheet.append(i)
print(rows)
wb.save("Marvel.xlsx")

>>> [['美国队长', '钢铁侠', '蜘蛛侠'], ['是', '漫威', '宇宙', '经典', '人物']]
```

#### `Excel`文件读取

- 我们来读取刚刚写入内容的“Marvel.xlsx”文件。

```python
import openpyxl

wb = openpyxl.load_workbook('Marvel.xlsx')
sheet = wb['new title']
sheetname = wb.sheetnames
print(sheetname)
A1_cell = sheet['A1']
A1_value = A1_cell.value
print(A1_value)

>>> ['new title']
	漫威宇宙
```

- 程序打印出来的['new title']是工作表的名字；“漫威宇宙”是我们刚写入A1单元格的文字。
- 一行行来看这个读取Excel文件的代码：

```python
第1行：调用openpyxl.load_workbook()函数，打开“Marvel.xlsx”文件。
第2行：获取“Marvel.xlsx”工作簿中名为“new title”的工作表。
第3、4行：sheetnames是用来获取工作簿所有工作表的名字的。如果你不知道工作簿到底有几个工作表，就可以把工作表的名字都打印出来。
第5-7行：把“new title”工作表中A1单元格赋值给A1_cell，再利用单元格value属性，就能打印出A1单元格的值。
```

- 如果你对openpyxl模块感兴趣，想要有更深入的了解的话，推荐阅读openpyxl模块的官方文档：https://openpyxl.readthedocs.io/en/stable/

### 基础知识：`csv`写入与读取

#### `csv`文件写入

- 首先，我们要引用csv模块。因为Python自带了csv模块，所以我们不需要安装就能引用它。
- 你是不是会困惑，明明前面csv写入我们可以直接用open函数来写，为什么现在还要先引用csv模块？答案：直接运用别人写好的模块，比我们使用open()函数来读写，语法更简洁，功能更强大，待会你就能感受到。那么，何乐而不为？

```python
# 引用csv模块。
import csv

# 创建csv文件，我们要先调用open()函数，传入参数：文件名“demo.csv”、写入模式“w”、newline=''、encoding='utf-8'。
csv_file = open('demo.csv','w',newline='',encoding='utf-8')
```

- 然后，我们得创建一个新的csv文件，命名为“demo.csv”。
- “w”就是write，即文件写入模式，它会以覆盖原内容的形式写入新添加的内容。
- 加`newline=' '`参数的原因是，可以避免csv文件出现两倍的行距（就是能避免表格的行与行之间出现空白行）。加`encoding='utf-8'`，可以避免编码问题导致的报错或乱码。
- 创建完csv文件后，我们要借助csv.writer()函数来建立一个writer对象。

```python
# 引用csv模块。
import csv

# 调用open()函数打开csv文件，传入参数：文件名“demo.csv”、写入模式“w”、newline=''、encoding='utf-8'。
csv_file = open('demo.csv','w',newline='',encoding='utf-8')
# 用csv.writer()函数创建一个writer对象。
writer = csv.writer(csv_file)
```

- 那怎么往csv文件里写入新的内容呢？答案是——调用writer对象的writerow()方法。

```python
# 借助writerow()函数可以在csv文件里写入一行文字 "电影"和“豆瓣评分”。
writer.writerow(['电影','豆瓣评分'])
```

- 提醒：writerow()函数里，需要放入列表参数，所以我们得把要写入的内容写成列表。就像['电影','豆瓣评分']。
- 我们试着再写入两部电影的名字和其对应的豆瓣评分，最后关闭文件，就完成csv文件的写入了。

```python
# 引用csv模块。
import csv

# 调用open()函数打开csv文件，传入参数：文件名“demo.csv”、写入模式“w”、newline=''、encoding='utf-8'。
csv_file = open('demo.csv','w',newline='',encoding='utf-8')
# 用csv.writer()函数创建一个writer对象。
writer = csv.writer(csv_file)
# 调用writer对象的writerow()方法，可以在csv文件里写入一行文字 “电影”和“豆瓣评分”。
writer.writerow(['电影','豆瓣评分'])
# 在csv文件里写入一行文字 “银河护卫队”和“8.0”。
writer.writerow(['银河护卫队','8.0'])
# 在csv文件里写入一行文字 “复仇者联盟”和“8.1”。
writer.writerow(['复仇者联盟','8.1'])
# 写入完成后，关闭文件就大功告成啦！
csv_file.close()
```

- 运行代码后，名为“demo.csv”的文件会被创建。用Excel或记事本打开这个文件，你就能看到——

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\CSV.png)

#### `csv`文件读取

- 以刚刚创建好的“demo.csv”文件为例。你可以先运行下面的代码，看看会读取出什么结果。

```python
import csv
csv_file=open('demo.csv','r',newline='',encoding='utf-8')
reader=csv.reader(csv_file)
for row in reader:
    print(row)
csv_file.close()

>>> ['电影', '豆瓣评分']
	['银河护卫队', '8.0']
	['复仇者联盟', '8.1']
```

- 现在，我们一行行来看刚刚读取“demo.csv”文件的代码:

```python
第1、2行：导入csv模块。用open()打开“demo.csv”文件，'r'是read读取模式，newline=''是避免出现两倍行距。encoding='utf-8'能避免编码问题导致的报错或乱码。
第3行：用csv.reader()函数创建一个reader对象。
第4、5行代码：用for循环遍历reader对象的每一行。打印row，就能读取出“demo.csv”文件里的内容。
```

- 补充一点：csv模块本身还有很多函数和方法，附上csv模块官方文档链接：https://yiyibooks.cn/xx/python_352/library/csv.html#module-csv

# `post`请求

- 其实，post和get都可以带着参数请求，不过get请求的参数会在url上显示出来。
- 但post请求的参数就不会直接显示，而是隐藏起来。像账号密码这种私密的信息，就应该用post的请求。如果用get请求的话，账号密码全部会显示在网址上，这显然不科学！你可以这么理解，get是明文显示，post是非明文显示。
- 通常，get请求会应用于获取网页数据，比如我们之前学的requests.get()。post请求则应用于向网页提交数据，比如提交表单类型数据（像账号密码就是网页表单的数据）。
- get和post是两种最常用的请求方式，除此之外，还有其他类型的请求方式，如head、options等。
- 现在，get和post这两种请求方式的区别弄懂了吧？我们继续往下看——：

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\post请求.png)

- 【requests headers】存储的是浏览器的请求信息，【response headers】存储的是服务器的响应信息。
- 你会看到在【response headers】里有set cookies的参数。set cookies是什么意思？就是服务器往浏览器写入了cookies。

## `cookies及其用法`

- 其实，你对cookies并不陌生，我敢肯定你见过它。比如一般当你登录一个网站，你都会在登录页面看到一个可勾选的选项“记住我”，如果你勾选了，以后你再打开这个网站就会自动登录，这就是cookie在起作用。
- 当你登录博客账号spiderman，并勾选“记住我”，服务器就会生成一个cookies和spiderman这个账号绑定。接着，它把这个cookies告诉你的浏览器，让浏览器把cookies存储到你的本地电脑。当下一次，浏览器带着cookies访问博客，服务器会知道你是spiderman，你不需要再重复输入账号密码，即可直接访问。

- 当然，cookies也是有时效性的，过期后就会失效。你应该有过这样的体验：哪怕勾选了“记住我”，但一段时间过去了，网站还是会提示你要重新登录，就是之前的cookies已经失效。
- 我们继续看【headers】,看看还有没有哪些有关登录的参数。
- 咦，拉到【form data】，可以看到5个参数：

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\cookies.png)

- log和pwd显然是我们的账号和密码，wp-submit猜一下就知道是登录的按钮，redirect_to后面带的链接是我们登录后会跳转到的这个页面网址，testcookie我们不知道是什么。
- 关于登录的参数我们找到了。现在可以尝试开始写代码，向服务器发起登录请求。

```python
import requests
#引入requests。
url = ' https://wordpress-edu-3autumn.localprod.oc.forchange.cn/wp-login.php'
#把登录的网址赋值给url。
headers = {
'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36'
}
#加请求头，前面有说过加请求头是为了模拟浏览器正常的访问，避免被反爬虫。
data = {
'log': 'spiderman',  #写入账户
'pwd': 'crawler334566',  #写入密码
'wp-submit': '登录',
'redirect_to': 'https://wordpress-edu-3autumn.localprod.oc.forchange.cn',
'testcookie': '1'
}
#把有关登录的参数封装成字典，赋值给data。
login_in = requests.post(url,headers=headers,data=data)
#用requests.post发起请求，放入参数：请求登录的网址、请求头和登录参数，然后赋值给login_in。
print(login_in)
#打印login_in

>>> <Response [200]>
```

- Response [200]，是返回了200的状态码，意味着服务器接收到并响应了登录请求。
- 不过，我们的目标是要往博客的文章里发表评论，所以成功登录只是第一步。
- 怎么发表评论我们现在还不知道。那就先分析看看“正常人”发表评论，浏览器会发送什么请求。
- 行，我们在《未来已来（一）——技术变革》这篇文章下面自己写一条评论发表（记得不要关闭检查工具，这样才能看到请求的记录）。
- 我按“正常人”的操作写了一条“纯属测试”的评论，点击发表。
- Network里迅速加载出很多请求，点开【wp-comments-post.php】，看headers，发现我刚刚发表的评论就藏在这里。

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\请求.png)

- comment是评论内容，submit是发表评论的按钮，另外两个参数我们看不懂，不过没关系，我们知道它们都是和评论有关的参数就行。
- 还会发现【wp-comments-post.php】的数据并没有藏在XHR中，而是放在了Other里。原因是我们搭建网站时就写在了Other里，但常规情况下，大部分网站都会把这样的数据存储在XHR里，比如知乎的回答。

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\other.png)

- 我们想要发表博客评论，首先得登录，其次得提取和调用登录的cookies，然后还需要评论的参数，才能发起评论的请求。
- 现在，登录的代码我们前面写好了，评论的参数我们刚也找到了，就差提取和调用登录的cookies。

- 我会先带你写一遍发表评论的代码（要认真看注释）：

```python
import requests
#引入requests。
url = ' https://wordpress-edu-3autumn.localprod.oc.forchange.cn/wp-login.php'
#把请求登录的网址赋值给url。
headers = {
'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36'
}
#加请求头，前面有说过加请求头是为了模拟浏览器正常的访问，避免被反爬虫。
data = {
'log': 'spiderman',  #写入账户
'pwd': 'crawler334566',  #写入密码
'wp-submit': '登录',
'redirect_to': 'https://wordpress-edu-3autumn.localprod.oc.forchange.cn',
'testcookie': '1'
}
#把有关登录的参数封装成字典，赋值给data。
login_in = requests.post(url,headers=headers,data=data)
#用requests.post发起请求，放入参数：请求登录的网址、请求头和登录参数，然后赋值给login_in。
cookies = login_in.cookies
#提取cookies的方法：调用requests对象（login_in）的cookies属性获得登录的cookies，并赋值给变量cookies。

url_1 = 'https://wordpress-edu-3autumn.localprod.oc.forchange.cn/wp-comments-post.php'
#我们想要评论的文章网址。
data_1 = {
'comment': input('请输入你想要发表的评论：'),
'submit': '发表评论',
'comment_post_ID': '13',
'comment_parent': '0'
}
#把有关评论的参数封装成字典。
comment = requests.post(url_1,headers=headers,data=data_1,cookies=cookies)
#用requests.post发起发表评论的请求，放入参数：文章网址、headers、评论参数、cookies参数，赋值给comment。
#调用cookies的方法就是在post请求中传入cookies=cookies的参数。
print(comment.status_code)
#打印出comment的状态码，若状态码等于200，则证明我们评论成功。
```

- 提取cookies的方法{`cookies = login_in.cookies`}：调用requests对象的cookies属性获得登录的cookies。
- 调用cookies的方法请看倒数第二行的代码：在post请求中传入cookies=cookies的参数。
- 最后之所以加一行打印状态码的代码，是想运行整个代码后，能立马判断出评论到底有没有成功发表。只要状态码等于200，就说明服务器成功接收并响应了我们的评论请求。
- 多解释一句：登录的cookies其实包含了很多名称和值，真正能帮助我们发表评论的cookies，只是取了登录cookies中某一小段值而已。所以登录的cookies和评论成功后，你在【wp-comments-post.php】里的headers面板中看到的cookies是不一致的。

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\cookies_1.png)

- 总结一下：发表博客评论就三个重点——

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\三个重点.png)

- 刷新文章的页面，你应该能找到自己的评论。
- 虽然我们已经成功发表了评论，但我们的项目到这里还没有结束。因为这个代码还有优化的空间（仅仅是完成还不够，更优雅才是我们该有的追求）。
- 如果要继续优化这个代码的话，我们需要理解一个新的概念——session（会话）。

## `session`及其用法

- 所谓的会话，你可以理解成我们用浏览器上网，到关闭浏览器的这一过程。session是会话过程中，服务器用来记录特定用户会话的信息。
- 比如你打开浏览器逛购物网页的整个过程中，浏览了哪些商品，在购物车里放了多少件物品，这些记录都会被服务器保存在session中。

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\session.png)

- 如果没有session，可能会出现这样搞笑的情况：你加购了很多商品在购物车，打算结算时，发现购物车空无一物Σ(っ°Д°;)っ，因为服务器根本没有帮你记录你想买的商品。
- 对了，session和cookies的关系还非常密切——cookies中存储着session的编码信息，session中又存储了cookies的信息。
- 当浏览器第一次访问购物网页时，服务器会返回set cookies的字段给浏览器，而浏览器会把cookies保存到本地。
- 等浏览器第二次访问这个购物网页时，就会带着cookies去请求，而因为cookies里带有会话的编码信息，服务器立马就能辨认出这个用户，同时返回和这个用户相关的特定编码的session。
- 这也是为什么你每次重新登录购物网站后，你之前在购物车放入的商品并不会消失的原因。因为你在登录时，服务器可以通过浏览器携带的cookies，找到保存了你购物车信息的session。
- 呼，session的概念，以及和cookies的关系我们搞清楚了，终于可以开始优化发表博客评论的代码。
- 既然cookies和session的关系如此密切，那我们可不可以通过创建一个session来处理cookies？
- 不知道。那就翻阅requests的官方文档找找看有没有这样的方法，能让我们创建session来处理cookies。

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\session_`.png)

- 在requests的高级用法里，还真有这样的方法，太棒了！

```python
import requests
#引用requests。
session = requests.session()
#用requests.session()创建session对象，相当于创建了一个特定的会话，帮我们自动保持了cookies。
url = 'https://wordpress-edu-3autumn.localprod.oc.forchange.cn/wp-login.php'
headers = {
'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'
}
data = {
    'log':input('请输入账号：'), #用input函数填写账号和密码，这样代码更优雅，而不是直接把账号密码填上去。
    'pwd':input('请输入密码：'),
    'wp-submit':'登录',
    'redirect_to':'https://wordpress-edu-3autumn.localprod.oc.forchange.cn',
    'testcookie':'1'
}
session.post(url,headers=headers,data=data)
#在创建的session下用post发起登录请求，放入参数：请求登录的网址、请求头和登录参数。

url_1 = 'https://wordpress-edu-3autumn.localprod.oc.forchange.cn/wp-comments-post.php'
#把我们想要评论的文章网址赋值给url_1。
data_1 = {
'comment': input('请输入你想要发表的评论：'),
'submit': '发表评论',
'comment_post_ID': '13',
'comment_parent': '0'
}
#把有关评论的参数封装成字典。
comment = session.post(url_1,headers=headers,data=data_1)
#在创建的session下用post发起评论请求，放入参数：文章网址，请求头和评论参数，并赋值给comment。
print(comment)
#打印comment
```

- 这么一细看，其实这个代码并没有特别大的优化，我们每次还是需要输入账号密码登录，才能发表评论。
- 可不可以有更优化的方案？
- 答案：可以有！cookies能帮我们保存登录的状态，那我们就在第一次登录时把cookies存储下来，等下次登录再把存储的cookies读取出来，这样就不用重复输入账号密码了。

## 存储`cookies`

- 我们先把登录的cookies打印出来看看，请点击运行下面的代码（账号：spiderman;密码：crawler334566）。

```python
import requests
session = requests.session()
url = 'https://wordpress-edu-3autumn.localprod.oc.forchange.cn/wp-login.php'
headers = {
'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'
}
data = {
    'log':input('请输入账号：'),
    'pwd':input('请输入密码：'),
    'wp-submit':'登录',
    'redirect_to':'https://wordpress-edu-3autumn.localprod.oc.forchange.cn',
    'testcookie':'1'
}
session.post(url,headers=headers,data=data)
print(type(session.cookies))
#打印cookies的类型,session.cookies就是登录的cookies
print(session.cookies)
#打印cookies

>>> 请输入账号：spiderman
	请输入密码：crawler334566
	<class 'requests.cookies.RequestsCookieJar'>
	<RequestsCookieJar[<Cookie 328dab9653f517ceea1f6dfce2255032=deeeeb9887e81e2580990177ea28687f for wordpress-edu-3autumn.localprod.oc.forchange.cn/>, <Cookie wordpress_test_cookie=WP+Cookie+check for wordpress-edu-3autumn.localprod.oc.forchange.cn/>]>
```

- RequestsCookieJar是cookies对象的类，cookies本身的内容有点像一个列表，里面又有点像字典的键与值，具体的值我们看不懂，也不需要弄懂。
- 那怎么把cookies存储下来？能不能用文件读写的方式，把cookies存储成txt文件？
- 可是txt文件存储的是字符串，刚刚打印出来的cookies并不是字符串。那有没有能把cookies转成字符串的方法？
- json模块能把字典转成字符串。我们或许可以先把cookies转成字典，然后再通过json模块转成字符串。这样，就能用open函数把cookies存储成txt文件。
- 感觉这样的思路应该可以实现。通过使用搜索引擎+翻阅官方文档的方式，就能找到了把cookies转化成字典的方法和json模块的使用方法。

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\转化.png)

- 把cookies存储成txt文件的代码如下（有注释的代码要认真看）：

```python
import requests,json
#引入requests和json模块。
session = requests.session()   
url = ' https://wordpress-edu-3autumn.localprod.oc.forchange.cn/wp-login.php'
headers = {
'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'
}
data = {
'log': input('请输入你的账号:'),
'pwd': input('请输入你的密码:'),
'wp-submit': '登录',
'redirect_to': 'https://wordpress-edu-3autumn.localprod.oc.forchange.cn',
'testcookie': '1'
}
session.post(url, headers=headers, data=data)

cookies_dict = requests.utils.dict_from_cookiejar(session.cookies)
#把cookies转化成字典。
print(cookies_dict)
#打印cookies_dict
cookies_str = json.dumps(cookies_dict)
#调用json模块的dumps函数，把cookies从字典再转成字符串。
print(cookies_str)
#打印cookies_str
f = open('cookies.txt', 'w')
#创建名为cookies.txt的文件，以写入模式写入内容。
f.write(cookies_str)
#把已经转成字符串的cookies写入文件。
f.close()
#关闭文件。
```

- 提示：以上存储cookies的方法并非最简单的方法，选取这个方法是因为它容易理解。如果你看完了，请运行代码（账号：spiderman;密码：crawler334566）。
- 运行代码后，确实证明了cookies可以被转成字典，也可以通过json模块把字典格式的cookies转成字符串。
- 这样一来，cookies的存储我们搞定了，但还得搞定cookies的读取，才能解决每次发表评论都得先输入账号密码的问题。

## 读取`cookies`

- 我们存储cookies时，是把它先转成字典，再转成字符串。读取cookies则刚好相反，要先把字符串转成字典，再把字典转成cookies本来的格式。
- 读取cookies的代码如下：

```python
cookies_txt = open('cookies.txt', 'r')
#以reader读取模式，打开名为cookies.txt的文件。
cookies_dict = json.loads(cookies_txt.read())
#调用json模块的loads函数，把字符串转成字典。
cookies = requests.utils.cookiejar_from_dict(cookies_dict)
#把转成字典的cookies再转成cookies本来的格式。
session.cookies = cookies
#获取cookies：就是调用requests对象（session）的cookies属性。
```

- 终于，cookies的存储与读取我们都弄好了。
- 最后我们可以把代码优化成：如果程序能读取到cookies，就自动登录，发表评论；如果读取不到，就重新输入账号密码登录，再评论。
- 再一次优化的代码如下：

```python
import requests,json
session = requests.session()
#创建会话。
headers = {
'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36'
}
#添加请求头，避免被反爬虫。
try:
#如果能读取到cookies文件，执行以下代码，跳过except的代码，不用登录就能发表评论。
    cookies_txt = open('cookies.txt', 'r')
    #以reader读取模式，打开名为cookies.txt的文件。
    cookies_dict = json.loads(cookies_txt.read())
    #调用json模块的loads函数，把字符串转成字典。
    cookies = requests.utils.cookiejar_from_dict(cookies_dict)
    #把转成字典的cookies再转成cookies本来的格式。
    session.cookies = cookies
    #获取cookies：就是调用requests对象（session）的cookies属性。

except FileNotFoundError:
#如果读取不到cookies文件，程序报“FileNotFoundError”（找不到文件）的错，则执行以下代码，重新登录获取cookies，再评论。

    url = ' https://wordpress-edu-3autumn.localprod.oc.forchange.cn/wp-login.php'
    #登录的网址。
    data = {'log': input('请输入你的账号:'),
            'pwd': input('请输入你的密码:'),
            'wp-submit': '登录',
            'redirect_to': 'https://wordpress-edu-3autumn.localprod.oc.forchange.cn',
            'testcookie': '1'}
    #登录的参数。
    session.post(url, headers=headers, data=data)
    #在会话下，用post发起登录请求。

    cookies_dict = requests.utils.dict_from_cookiejar(session.cookies)
    #把cookies转化成字典。
    cookies_str = json.dumps(cookies_dict)
    #调用json模块的dump函数，把cookies从字典再转成字符串。
    f = open('cookies.txt', 'w')
    #创建名为cookies.txt的文件，以写入模式写入内容
    f.write(cookies_str)
    #把已经转成字符串的cookies写入文件
    f.close()
    #关闭文件

url_1 = 'https://wordpress-edu-3autumn.localprod.oc.forchange.cn/wp-comments-post.php'
#文章的网址。
data_1 = {
'comment': input('请输入你想评论的内容：'),
'submit': '发表评论',
'comment_post_ID': '13',
'comment_parent': '0'
}
#评论的参数。
comment = session.post(url_1,headers=headers,data=data_1)
#在创建的session下用post发起评论请求，放入参数：文章网址，请求头和评论参数，并赋值给comment。
print(comment.status_code)
#打印comment的状态码
```

- 这样是解决了每一次都要重复输入账号密码的问题，但这个代码还存在一个缺陷——并没有解决cookies会过期的问题。
- cookies是否过期，我们可以通过最后的状态码是否等于200来判断。但更好的解决方法应该在代码里加一个条件判断，如果cookies过期，就重新获取新的cookies。
- 所以，更完整以及面向对象的代码应该是下面这样的：

```python
import requests, json
session = requests.session()
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36'}

def cookies_read():
    cookies_txt = open('cookies.txt', 'r')
    cookies_dict = json.loads(cookies_txt.read())
    cookies = requests.utils.cookiejar_from_dict(cookies_dict)
    return (cookies)
    # 以上4行代码，是cookies读取。

def sign_in():
    url = ' https://wordpress-edu-3autumn.localprod.oc.forchange.cn/wp-login.php'
    data = {'log': input('请输入你的账号'),
            'pwd': input('请输入你的密码'),
            'wp-submit': '登录',
            'redirect_to': 'https://wordpress-edu-3autumn.localprod.oc.forchange.cn',
            'testcookie': '1'}
    session.post(url, headers=headers, data=data)
    cookies_dict = requests.utils.dict_from_cookiejar(session.cookies)
    cookies_str = json.dumps(cookies_dict)
    f = open('cookies.txt', 'w')
    f.write(cookies_str)
    f.close()
    # 以上5行代码，是cookies存储。


def write_message():
    url_2 = 'https://wordpress-edu-3autumn.localprod.oc.forchange.cn/wp-comments-post.php'
    data_2 = {
        'comment': input('请输入你要发表的评论：'),
        'submit': '发表评论',
        'comment_post_ID': '13',
        'comment_parent': '0'
    }
    return (session.post(url_2, headers=headers, data=data_2))
    #以上9行代码，是发表评论。

try:
    session.cookies = cookies_read()
except FileNotFoundError:
    sign_in()

num = write_message()
if num.status_code == 200:
    print('成功啦！')
else:
    sign_in()
    num = write_message()
```

- 最后，还想和你多说几句——
- 其实，计算机之所以需要cookies和session，是因为HTTP协议是无状态的协议.
- 何为无状态？就是一旦浏览器和服务器之间的请求和响应完毕后，两者会立马断开连接，也就是恢复成无状态。
- 这样会导致：服务器永远无法辨认，也记不住用户的信息，像一条只有7秒记忆的金鱼。是cookies和session的出现，才破除了web发展史上的这个难题。
- cookies不仅仅能实现自动登录，因为它本身携带了session的编码信息，网站还能根据cookies，记录你的浏览足迹，从而知道你的偏好，只要再加以推荐算法，就可以实现给你推送定制化的内容。
- 比如，淘宝会根据你搜索和浏览商品的记录，给你推送符合你偏好的商品，增加你的购买率。cookies和session在这其中起到的作用，可谓举足轻重。

# `selenium`是什么

- `selenium`是什么呢？它是一个强大的Python库。
- 它可以做什么呢？它可以用几行代码，控制浏览器，做出自动打开、输入、点击等操作，就像是有一个真正的用户在操作一样。
- `selenium`能控制浏览器，这对解决我们刚刚提出的那几个问题，有什么帮助呢？
- 首先，当你遇到验证码很复杂的网站时，`selenium`允许让人去手动输入验证码，然后把剩下的操作交给机器。
- 而对于那些交互复杂、加密复杂的网站，`selenium`问题简化，爬动态网页如爬静态网页一样简单。
- 什么是动态网页，什么又是静态网页呢？其实两种网页你都已经接触过了。
- 教你用`html`写出的网页，就是静态网页。我们使用`BeautifulSoup`爬取这类型网页，因为网页源代码中就包含着网页的所有信息，因此，网页地址栏的`URL`就是网页源代码的`URL`。

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\jing.png)

- 后来，你开始接触更复杂的网页，比如QQ音乐，要爬取的数据不在HTML源代码中，而是在`json`中，你就不能直接使用网址栏的`URL`了，而需要找到`json`数据的真实`URL`。这就是一种动态网页。

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\jing1.png)

- 不论数据存在哪里，浏览器总是在向服务器发起各式各样的请求，当这些请求完成后，它们会一起组成开发者工具的`Elements`中所展示的，渲染完成的网页源代码。
- 在遇到页面交互复杂或是`URL`加密逻辑复杂的情况时，`selenium`就派上了用场，它可以真实地打开一个浏览器，等待所有数据都加载到`Elements`中之后，再把这个网页当做静态网页爬取就好了。
- 说了这么多优点，使用`selenium`时，当然也有美中不足之处。
- 由于要真实地运行本地浏览器，打开浏览器以及等待网渲染完成需要一些时间，`selenium`的工作不可避免地牺牲了速度和更多资源，不过，至少不会比人慢。

## selenium`怎么使用

- 在正式开始知识的讲解之前，我想首先让你体验一下`selenium`脚本程序在你的本地终端运行的效果。因为在学习`selenium`之初，如果能亲自看到浏览器自动弹出后的操作效果，对你后续的学习会有很大帮助。
- 现在只需要把这段代码复制到本地的代码编辑器中运行，体验一下你的浏览器为你自动工作的效果。当然，前提是你已经安装好了`selenium`库以及Chrome浏览器驱动。

### 设置浏览器引擎

- 和以前一样，使用一个新的Python库，首先要调用它。`selenium`有点不同，除了调用，还需要设置浏览器引擎。

```python
# 本地Chrome浏览器设置方法
from selenium import webdriver #从selenium库中调用webdriver模块
driver = webdriver.Chrome() # 设置引擎为Chrome，真实地打开一个Chrome浏览器
```

- 以上就是浏览器的设置方式：把Chrome浏览器设置为引擎，然后赋值给变量`driver`。`driver`是实例化的浏览器，在后面你会总是能看到它的影子，这也可以理解，因为我们要控制这个实例化的浏览器为我们做一些事情。

### 获取数据

- 首先看一下获取数据的代码怎么写吧。

```python
import time

# 本地Chrome浏览器设置方法
from selenium import webdriver #从selenium库中调用webdriver模块
driver = webdriver.Chrome() # 设置引擎为Chrome，真实地打开一个Chrome浏览器

driver.get('https://localprod.pandateacher.com/python-manuscript/hello-spiderman/') # 打开网页
time.sleep(1)
driver.close() # 关闭浏览器
```

- 前面三行代码都是你学过的，调用模块，并且设置浏览器，只有后两行代码是新的。
- `get(URL)`是`webdriver`的一个方法，它的使命是为你打开指定URL的网页。
- 刚才说过`driver`在这里是一个实例化的浏览器，因此，就是通过这个浏览器打开网页。
- 当一个网页被打开，网页中的数据就加载到了浏览器中，也就是说，数据被我们获取到了。
- `driver.close()`是关闭浏览器驱动，每次调用了`webdriver`之后，都要在用完它之后加上一行`driver.close()`用来关闭它。
- 就像是，每次打开冰箱门，把东西放进去之后，都要记得关上门，使用`selenium`调用了浏览器之后也要记得关闭浏览器。
- 把上面的代码复制粘贴在你的本地电脑中运行，你可以看到，一个浏览器自动启动，并为你打开一个网页，停留一秒之后，浏览器关闭。

### 解析与提取数据

- `selenium`库同样也具备解析数据、提取数据的能力。它和`BeautifulSoup`的底层原理一致，但在一些细节和语法上有所出入。
- 首先明显的一个不同即是：`selenium`所解析提取的，是`Elements`中的所有数据，而`BeautifulSoup`所解析的则只是`Network`中第0个请求的响应。
- 用`selenium`把网页打开，所有信息就都加载到了`Elements`那里，之后，就可以把动态网页用静态网页的方法爬取了。
- `selenium`是如何解析与提取数据的呢？我们现在来试试提取【你好蜘蛛侠！】网页中，`<label>`元素的内容吧。

```python
import time
from selenium import webdriver #从selenium库中调用webdriver模块

driver = webdriver.Chrome() # 声明浏览器对象
driver.get('https://localprod.pandateacher.com/python-manuscript/hello-spiderman/') # 访问页面
time.sleep(3) # 等待3秒
label = driver.find_element_by_tag_name('label') # 解析网页并提取第一个<label>标签
print(label.text) # 打印label的文本
driver.close() # 关闭浏览器

>>> (提示：吴枫)
```

- 从运行结果中可以看到，我们提取出了`<label>（提示：吴枫）</label>`中的文本`（提示：吴枫）`。
- 上面这段代码只有最后几行代码是新增的，倒数第四行：等待2秒；倒数第三行：然后解析网页并提取网页中第一个`<label>`标签；倒数第二行：打印label的文本内容。
- 用`time.sleep(3)`等待三秒，是由于浏览器缓冲加载网页需要耗费一些时间，以及我在这个网页中设置了一秒之后才从首页跳转到输入页面，所以，等待三秒再去解析和提取比较稳妥。
- 这样来看，解析与提取数据，在这里其实只用了一行代码：

```python
label = driver.find_element_by_tag_name('label') # 解析网页并提取第一个<label>标签中的文字
```

- 你能否看出，是哪部分在做解析，哪部分在做提取？
- 先回想下，使用`BeautifulSoup`解析提取数据时，首先要把`Response`对象解析为`BeautifulSoup`对象，然后再从中提取数据。
- 而在`selenium`中，获取到的网页存在了`driver`中，而后，解析与提取是同时做的，都是由`driver`这个实例化的浏览器完成。
- 所以，上个问题的答案是：解析数据是由`driver`自动完成的，提取数据是`driver`的一个方法。
- 清楚了解析与提取的本质，我们接下来详细讲解析数据的方法。
- `selenium`当然不光能通过标签来提取数据，还有很多查找和提取元素的方法，都是非常直截了当的方法。

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\selenium.png)

- 你可以看出，提取数据的方法都是英文直译的意思。举例给你看看它们的用法，请仔细阅读下面代码的注释：

```python
# 以下方法都可以从网页中提取出'你好，蜘蛛侠！'这段文字

find_element_by_tag_name：通过元素的名称选择
# 如<h1>你好，蜘蛛侠！</h1> 
# 可以使用find_element_by_tag_name('h1')

find_element_by_class_name：通过元素的class属性选择
# 如<h1 class="title">你好，蜘蛛侠！</h1>
# 可以使用find_element_by_class_name('title')

find_element_by_id：通过元素的id选择
# 如<h1 id="title">你好，蜘蛛侠！</h1> 
# 可以使用find_element_by_id('title')

find_element_by_name：通过元素的name属性选择
# 如<h1 name="hello">你好，蜘蛛侠！</h1> 
# 可以使用find_element_by_name('hello')

#以下两个方法可以提取出超链接

find_element_by_link_text：通过链接文本获取超链接
# 如<a href="spidermen.html">你好，蜘蛛侠！</a>
# 可以使用find_element_by_link_text('你好，蜘蛛侠！')

find_element_by_partial_link_text：通过链接的部分文本获取超链接
# 如<a href="https://localprod.pandateacher.com/python-manuscript/hello-spiderman/">你好，蜘蛛侠！</a>
# 可以使用find_element_by_partial_link_text('你好')
```

- 以上就是提取单个元素的方法了。
- 那么，我们提取出的元素是什么类呢？这种对象有什么属性和方法呢？

```python
import time
from selenium import webdriver

driver = webdriver.Chrome() # 声明浏览器对象
driver.get('https://localprod.pandateacher.com/python-manuscript/hello-spiderman/') # 访问页面
time.sleep(3) # 等待三秒

label = driver.find_element_by_tag_name('label') # 解析网页并提取第一个<label>标签中的文字
print(type(label)) # 打印label的数据类型
print(label.text) # 打印label的文本
print(label) # 打印label
driver.close() # 关闭浏览器

>>> <class 'selenium.webdriver.remote.webelement.WebElement'>
	（提示：吴枫）
<selenium.webdriver.remote.webelement.WebElement (session="560da7dd24b8c46eea3b0304aafe1f47", element="03096c16-5b3b-4db0-9ca8-79713b617e20")>
```

- 运行结果有3行，分别是：
- `<class 'selenium.webdriver.remote.webelement.WebElement'>`、`label`的文本`（提示：吴枫）`、以及`label`本身。
- 可见，提取出的数据属于`WebElement`类对象，如果直接打印它，返回的是一串对它的描述。
- 而它与`BeautifulSoup`中的`Tag`对象类似，也有一个属性`.text`，可以把提取出的元素用字符串格式显示。
- 还想补充的是，`WebElement`类对象与`Tag`对象类似，它也有一个方法，可以通过属性名提取属性的值，这个方法是`.get_attribute()`。

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\selenium与tag对比.png)

- 我们试试，通过`class="teacher"`定位到上图中标亮的元素，然后提取出`type`这个属性的值`text`。

```python
# 教学系统的浏览器设置方法
from selenium import webdriver # 从selenium库中调用webdriver模块
from selenium.webdriver.chrome.options import Options # 从options模块中调用Options类
import time

chrome_options = Options() # 实例化Option对象
chrome_options.add_argument('--headless') # 对浏览器的设置
driver = webdriver.Chrome(options=chrome_options) # 声明浏览器对象

driver.get('https://localprod.pandateacher.com/python-manuscript/hello-spiderman/') # 访问页面
time.sleep(3) # 等待三秒

label = driver.find_element_by_class_name('teacher') # 根据类名找到元素
print(type(label)) # 打印label的数据类型
print(label.get_attribute('type')) # 获取type这个属性的值
driver.close() # 关闭浏览器

>>> <class 'selenium.webdriver.remote.webelement.WebElement'>
text
```

- 我们可以总结出，`selenium`解析与提取数据的过程中，我们操作的对象转换：

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\转换.png)

- 刚才，我们做的都是提取出网页中的第一个符合要求的数据，接下来，我们就看看提取多个元素的方法吧。
- `find_element_by_`与`BeautifulSoup`中的`find`类似，可以提取出网页中第一个符合要求的元素；既然`BeautifulSoup`有提取所有元素的方法`find_all`，`selenium`也同样有方法。
- 方法也一样很简单，把刚才的`element`换成复数`elements`就好了。

![](C:\Users\29678\Desktop\learn\ptyhon笔记\level_design\爬虫基础\提取多个数据的方法.png)

- 我们来试试提取出【你好,蜘蛛侠！】的所有`label`标签中的文字。

```python
# 教学系统的浏览器设置方法
from selenium import webdriver # 从selenium库中调用webdriver模块
from selenium.webdriver.chrome.options import Options # 从options模块中调用Options类
import time

chrome_options = Options() # 实例化Option对象
chrome_options.add_argument('--headless') # 对浏览器的设置
driver = webdriver.Chrome(options=chrome_options) # 声明浏览器对象

driver.get('https://localprod.pandateacher.com/python-manuscript/hello-spiderman/') # 访问页面
time.sleep(3) # 等待三秒

labels = driver.find_elements_by_tag_name('label') # 根据标签名提取所有元素
print(type(labels)) # 打印labels的数据类型
print(labels) # 打印labels
driver.close() # 关闭浏览器

>>> <class 'list'>
[<selenium.webdriver.remote.webelement.WebElement (session="a830d4e8361f0a41239e170db60cb4f2", element="8545a815-851f-43f2-b791-169a3a123355")>, <selenium.webdriver.remote.webelement.WebElement (session="a830d4e8361f0a41239e170db60cb4f2", element="54c1c671-23db-4997-953f-defab6fd2681")>]
```

- 从运行结果可以看到，提取出的是一个列表，`<class 'list'>`。而列表的内容就是`WebElements`对象，这些符号是对象的描述，我们刚才学过，需要用`.text`才能返回它的文本内容。
- 既然得到了列表，就可以和`find_all`返回的结果类似，同样用`for`循环遍历列表就可以提取出列表中的每一个值了。

```python
# 教学系统的浏览器设置方法，你不需要记住它
from selenium import webdriver # 从selenium库中调用webdriver模块
from selenium.webdriver.chrome.options import Options # 从options模块中调用Options类
import time

chrome_options = Options() # 实例化Option对象
chrome_options.add_argument('--headless') # 对浏览器的设置
driver = webdriver.Chrome(options=chrome_options) # 声明浏览器对象

driver.get('https://localprod.pandateacher.com/python-manuscript/hello-spiderman/') # 访问页面
time.sleep(3)

labels = driver.find_elements_by_tag_name('label') # 根据标签名提取所有元素
print(type(labels))  # 打印labels的数据类型
for label in labels: # 循环，遍历labels这个列表
    print(label.text) # 打印labe的文本
driver.close() # 关闭浏览器

>>> <class 'list'>
	（提示：吴枫）
	（提示：酱酱，延君，卡西，艾德）
```

- 以上就是`selenium`的解析与提取数据的方法了。
- 除了用`selenium`解析与提取数据，还有一种解决方案，那就是，使用`selenium`获取网页，然后交给`BeautifulSoup`解析和提取。
- 接下来，我们就看看，`selenium`与`BeautifulSoup`如何快乐地合作。
- `BeautifulSoup`需要把字符串格式的网页源代码解析为`BeautifulSoup`对象，然后再从中提取数据。
- `selenium`刚好可以获取到渲染完整的网页源代码。
- 如何获取呢？也是使用`driver`的一个方法：`page_source`。

```python
HTML源代码字符串 = driver.page_source 
```

- 我们现在就来实操一下，获取【你好，蜘蛛侠！】的网页源代码：

- 

- ```python
  # 教学系统的浏览器设置方法
  from selenium import webdriver # 从selenium库中调用webdriver模块
  from selenium.webdriver.chrome.options import Options # 从options模块中调用Options类
  import time
  
  chrome_options = Options() # 实例化Option对象
  chrome_options.add_argument('--headless') # 对浏览器的设置
  driver = webdriver.Chrome(options=chrome_options) # 声明浏览器对象
  
  driver.get('https://localprod.pandateacher.com/python-manuscript/hello-spiderman/') # 访问页面
  time.sleep(3) # 等待三秒，等浏览器加缓冲载数据
  
  pageSource = driver.page_source # 获取完整渲染的网页源代码
  print(type(pageSource)) # 打印pageSource的类型
  print(pageSource) # 打印pageSource
  driver.close() # 关闭浏览器
  
  >>> ...{太长了就不复制了}
  ```

- 我们成功获取并打印出了网页源代码O(∩_∩)O~~而且它的数据类型是`<class 'str'>`。
- 你还记不记得，用`requests.get()`获取到的是`Response`对象，在交给`BeautifulSoup`解析之前，需要用到`.text`的方法才能将`Response`对象的内容以字符串的形式返回。
- 而使用`selenium`获取到的网页源代码，本身已经是字符串了。
- 获取到了字符串格式的网页源代码之后，就可以用`BeautifulSoup`解析和提取数据了，这是我留给你的一个课后作业。
- 到这里，解析与提取数据的方法就讲解完了。
- 关于`selenium`的用法，还有什么没有讲呢？对！就是我们在本关开头演示的功能，控制浏览器自动输入文本，并且点击提交。

### 自动操作浏览器

- 其实，要做到上面动图中显示的效果，你只需要新学两个方法就好了：

```python
.send_keys() # 模拟按键输入，自动填写表单
.click() # 点击元素
```

- 用这两行代码，再搭配刚才所讲的解析提取数据的方法，就可以完成操作浏览器的效果了。
- 学到这里，我们就可以写下全部代码了，这也正是我在开头给你的，让你复制到本地运行过的代码。

```python
# 本地Chrome浏览器设置方法
from selenium import webdriver # 从selenium库中调用webdriver模块
import time # 调用time模块
driver = webdriver.Chrome() # 设置引擎为Chrome，真实地打开一个Chrome浏览器

driver.get('https://localprod.pandateacher.com/python-manuscript/hello-spiderman/') # 访问页面
time.sleep(3) # 暂停三秒，等待浏览器缓冲

teacher = driver.find_element_by_id('teacher') # 找到【请输入你喜欢的老师】下面的输入框位置
teacher.send_keys('必须是吴枫呀') # 输入文字
assistant = driver.find_element_by_name('assistant') # 找到【请输入你喜欢的助教】下面的输入框位置
assistant.send_keys('都喜欢') # 输入文字
button = driver.find_element_by_class_name('sub') # 找到【提交】按钮
button.click() # 点击【提交】按钮
time.sleep(1)
driver.close() # 关闭浏览器
```

- 重点关注最后的8行代码，这段代码所做的是两次输入以及一次点击的操作，然后等待一秒，关闭浏览器驱动。
- 在本地的操作环境中，你还可以把自己电脑中的Chrome浏览器设置为静默模式，也就是说，让浏览器只是在后台运行，并不在电脑中打开它的可视界面。
- 因为在做爬虫时，通常不需要打开浏览器，爬虫的目的是爬到数据，而不是观看浏览器的操作过程，在这种情况下，就可以使用浏览器的静默模式。
- 它的设置方法是这样的：

```python
# 本地Chrome浏览器的静默模式设置：
from selenium import  webdriver #从selenium库中调用webdriver模块
from selenium.webdriver.chrome.options import Options # 从options模块中调用Options类

chrome_options = Options() # 实例化Option对象
chrome_options.add_argument('--headless') # 把Chrome浏览器设置为静默模式
driver = webdriver.Chrome(options = chrome_options) # 设置引擎为Chrome，在后台默默运行
```

- 与上面浏览器的可视设置相比，3、5、6行代码是新增的，首先调用了一个新的类——`Options`，然后通过它的方法和属性，给浏览器输入了一个参数——`headless`。第7行代码中，把刚才所做的浏览器设置传给了Chrome浏览器。

- 到这里，你应该能感受到，Selenium是一个强大的网络数据采集工具，它的优势是简单直观，而它当然也有缺点。

- 由于是真实地模拟人操作浏览器，需要等待网页缓冲的时间，在爬取大量数据的时候，速度会比较慢。

- 通常情况，在爬虫项目中，`selenium`都是用在其它方法无法解决，或是很难解决的问题时，才会用到。

- 当然，除了爬虫，`selenium`的使用场景还有很多。比如：它可以控制网页中图片文件的显示、控制CSS和JavaScript的加载与执行等等。

- 我们的课程只是带你入门，讲了一些简单常用的操作，还想进一步学习的话，可以通过selenium的官方文档链，目前只有英文版:

- https://seleniumhq.github.io/selenium/docs/api/py/api.html

- 还可以参考这个中文文档：

- https://selenium-python-zh.readthedocs.io/en/latest/

  
